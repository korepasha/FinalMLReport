{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Report on Final Project</h1>\n",
    "<h4 align=\"center\">Korepanova Natalia</h4>\n",
    "<h5 align=\"center\">Moscow, 2015</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "**Challenges in Representation Learning: Facial Expression Recognition Challenge, Kaggle, \n",
    "Fri 12 Apr 2013 â€“ Fri 24 May 2013** https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "\n",
    "* 48x48 pixel grayscale images of faces (about 300 mb).\n",
    "* The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image.\n",
    "* The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
    "* 28709 train examples, two train sets - each of them contains 3589 entries.\n",
    "* Accuracy is used for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark experiments\n",
    "\n",
    "**Naive Bayes** attempts were not successful. Accuraccy on the test and train set was about 0.25.\n",
    "\n",
    "**Logistic regression with LBFGS optimizer with different parameters** were tried (uncluding pre-transformation with PCA). Finally, the best score on the whole test set was optained when\n",
    "\n",
    "* the number of iterations is limited by 500\n",
    "* L2 regularization parameter is 0.1\n",
    "* intercept is fitted\n",
    "* image colors was inversed (255 - pixel_value)\n",
    "* the data was standardized along each pixel\n",
    "* the model is trained against 0 class\n",
    "\n",
    "*Train accuracy: 0.3994, Test accuracy: 0.3845*\n",
    "\n",
    "Class|Precision|Recall\n",
    "-|-|-|-\n",
    "0|0.2720|0.0741\n",
    "1|0.0|0.0\n",
    "2|0.2796|0.1474\n",
    "3|0.4601|0.6741\n",
    "4|0.3148|0.3416\n",
    "5|0.4191|0.5523\n",
    "6|0.3436|0.3706\n",
    "\n",
    "* Weighted precision: 0.3521\n",
    "* Weighted recall: 0.3845\n",
    "* Weighted F1 score: 0.3516\n",
    "\n",
    "**Decision Tree and Ensembles** were tried but all attempts were resulted in memory error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theanets experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natalia/anaconda3/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import theanets as tn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./fer2013/fer2013.csv\")\n",
    "pixels = df[\"pixels\"]\n",
    "df = df.drop(\"pixels\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images are scaled to [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.zeros((df.shape[0], 48*48), dtype=float)\n",
    "for i in range(df.shape[0]):\n",
    "    X[i, :] = [float(x)  for x in pixels.iloc[i].split(\" \")]\n",
    "    xmin = np.amin(X[i, :])\n",
    "    xmax = np.amax(X[i, :])\n",
    "    if xmax-xmin != 0:\n",
    "        X[i, :] = (X[i, :] - xmin)*(1 - 0)/(xmax - xmin) + 0\n",
    "    else:\n",
    "        X[i, :] = 1\n",
    "del pixels\n",
    "Xdf = pd.DataFrame(X)\n",
    "del X\n",
    "df = pd.concat([df, Xdf], axis=1)\n",
    "del Xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 2305) (7178, 2305)\n"
     ]
    }
   ],
   "source": [
    "traindf = df[df['Usage'] == 'Training'].drop('Usage', axis=1)\n",
    "testdf = df[df['Usage'] != 'Training'].drop('Usage', axis=1)\n",
    "del df\n",
    "print(traindf.shape, testdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = traindf.drop(\"emotion\", axis=1).as_matrix().astype('f'), traindf['emotion'].as_matrix().astype('i')\n",
    "test = testdf.drop(\"emotion\", axis=1).as_matrix().astype('f'), testdf['emotion'].as_matrix().astype('i')\n",
    "del traindf\n",
    "del testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion:  0 angry\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDBm9EIUhoxeiMP4fjkODr08BYuAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc92f8056d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = train[1] == 0\n",
    "plt.matshow(train[0][idx][0, :].reshape(48, 48), cmap=plt.cm.gray)\n",
    "print(\"Emotion: \", 0, \"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion:  1 disgust\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc92f8fb4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = train[1] == 1\n",
    "plt.matshow(train[0][idx][0, :].reshape(48, 48), cmap=plt.cm.gray)\n",
    "print(\"Emotion: \", 1, \"disgust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion:  2 fear\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noxeiMGT0QhSGjF6Iwvg/WXGF3fVtxhIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc92f6d5c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = train[1] == 2\n",
    "plt.matshow(train[0][idx][0, :].reshape(48, 48), cmap=plt.cm.gray)\n",
    "print(\"Emotion: \", 2, \"fear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion:  3 happy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvRCFIaMXojD+D8dh9bs3MUDBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc92f747a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = train[1] == 3\n",
    "plt.matshow(train[0][idx][0, :].reshape(48, 48), cmap=plt.cm.gray)\n",
    "print(\"Emotion: \", 3, \"happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion:  4 sad\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyrub4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc92b6aa9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = train[1] == 4\n",
    "plt.matshow(train[0][idx][0, :].reshape(48, 48), cmap=plt.cm.gray)\n",
    "print(\"Emotion: \", 4, \"sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion:  5 suprise\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc92b6b70f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = train[1] == 5\n",
    "plt.matshow(train[0][idx][0, :].reshape(48, 48), cmap=plt.cm.gray)\n",
    "print(\"Emotion: \", 5, \"suprise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion:  6 neutral\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYcjohSgMGb0QhSGjF6IwZPRCFMb/A+6lcNNrcKhHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc928be2518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = train[1] == 6\n",
    "plt.matshow(train[0][idx][0, :].reshape(48, 48), cmap=plt.cm.gray)\n",
    "print(\"Emotion: \", 6, \"neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **1 hidden layer with 5 hidden nodes**\n",
    "* **all activation fincations are 'relu'**\n",
    "* **SGD with learning rate 1e-3 and momentum 0.9**\n",
    "* **L2 regularization with parameter value 0.01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.81658052772 0.255080734967\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.75068574175 0.29450863029\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.70889467948 0.321701002227\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.6951251051 0.331987750557\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.68699002585 0.336685690423\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.67842772632 0.339170378619\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.66961043026 0.344376391982\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.66729313147 0.344703507795\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.663984662 0.346533964365\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.65696005912 0.348587138085\n",
      "most recent validation loss: 2.170755237 0.0154166666667\n",
      "training loss: 1.65571604792 0.349227449889\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.65296458243 0.350654231626\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.64812828349 0.353716592428\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.64686299938 0.353438195991\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.64012122503 0.357196547884\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.64237020378 0.355595768374\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.63922839958 0.358880846325\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.63839191554 0.361302895323\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.63285141916 0.360558184855\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.63180318786 0.362138084633\n",
      "most recent validation loss: 1.65966673311 0.349527777778\n",
      "training loss: 1.62984633028 0.362625278396\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.63060951334 0.362813195991\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.62728242496 0.363008073497\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.63038805154 0.363460467706\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.62914544498 0.361045378619\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.62226659858 0.366021714922\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.62312067142 0.366849944321\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.62284393238 0.365917316258\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.62165734139 0.364226057906\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.62093202026 0.364594933185\n",
      "most recent validation loss: 1.64587602464 0.352611111111\n",
      "training loss: 1.61924522593 0.366870824053\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.61919144047 0.366209632517\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.61607372449 0.368923997773\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.61592601448 0.369780066815\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.61544666927 0.367170100223\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.61552514136 0.368910077951\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.61515018026 0.366404510022\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.61389271536 0.367636414254\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.61478350869 0.368074888641\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.61199919371 0.368854398664\n",
      "most recent validation loss: 1.64500430449 0.350638888889\n",
      "training loss: 1.60831287154 0.371687082405\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.6097853598 0.37058045657\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.61008680567 0.370942371938\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.60774489026 0.369828786192\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.60680915837 0.371812360802\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.60870597372 0.372661469933\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.6051939371 0.373517538976\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.60537650273 0.371067650334\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.60533695137 0.371380846325\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.60337064177 0.371881959911\n",
      "most recent validation loss: 1.64098936307 0.361638888889\n",
      "training loss: 1.60192268578 0.372946826281\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.60618755463 0.372891146993\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.60466369968 0.374596325167\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.60199912354 0.373503619154\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.60469374015 0.373148663697\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.60052467009 0.375069599109\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.60480203521 0.370302060134\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.59994369395 0.37614142539\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.60137428817 0.373983853007\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.59944665334 0.373851614699\n",
      "most recent validation loss: 1.62670101846 0.359\n",
      "training loss: 1.59778685781 0.376997494432\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.60012008564 0.373287861915\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.59974484215 0.374839922049\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.59734097586 0.374123051225\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.59849674964 0.375626391982\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.59610693567 0.376879175947\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.59694064334 0.376176224944\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.59684238436 0.372424832962\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.59435649135 0.376879175947\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.59629249296 0.376162305122\n",
      "most recent validation loss: 1.63645322275 0.356055555556\n",
      "training loss: 1.5954922844 0.375939587973\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.5936184729 0.377227171492\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.59265907694 0.37690701559\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.59337114877 0.37660077951\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.59435712817 0.37660077951\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.59480499879 0.378472995546\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.59332906677 0.375515033408\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.59610831818 0.374575445434\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.59245719425 0.375591592428\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.59145430578 0.37972577951\n",
      "most recent validation loss: 1.6459416944 0.350083333333\n",
      "training loss: 1.59332314124 0.376287583519\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.59541452634 0.376545100223\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.58960549682 0.379663140312\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.59198121727 0.377783964365\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.58964981049 0.378528674833\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.58770052913 0.380477449889\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.58777555096 0.37918986637\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.58874130339 0.378772271715\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.59002370632 0.378772271715\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.58868564747 0.378807071269\n",
      "most recent validation loss: 1.62663280074 0.362055555556\n",
      "training loss: 1.58807499936 0.377679565702\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.58723022452 0.378194599109\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.58768201804 0.379781458797\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.5861902891 0.380512249443\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.58557134189 0.377937082405\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.58556677386 0.378194599109\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.58569794753 0.382683741648\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.58729923675 0.376009187082\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.58460157915 0.380059855234\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.58652658596 0.37995545657\n",
      "most recent validation loss: 1.67892883562 0.332833333333\n",
      "training loss: 1.58406227755 0.379085467706\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.58409942403 0.379969376392\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.58447543308 0.378702672606\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.5838750397 0.378807071269\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.58422263266 0.379990256125\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.58518722056 0.376461581292\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.58655041582 0.377832683742\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.58120353683 0.383136135857\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.58282812357 0.381507516704\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.5831985282 0.37995545657\n",
      "most recent validation loss: 1.64378896332 0.359388888889\n",
      "training loss: 1.58267145335 0.37957266147\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.58308982021 0.381437917595\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.5817151544 0.380373051225\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.58128147705 0.381925111359\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.58070476121 0.380811525612\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.58363716716 0.377275890869\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.57993884225 0.381813752784\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.58041579668 0.38178591314\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.58134944622 0.378041481069\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.57886141291 0.380442650334\n",
      "most recent validation loss: 1.63505659476 0.359833333333\n",
      "training loss: 1.57810465595 0.383087416481\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.57874030874 0.37941954343\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.57796250499 0.384361080178\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.57924696248 0.381430957684\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.57947567802 0.383769487751\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.5791101959 0.378598273942\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.57630166942 0.381396158129\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.57943355719 0.37980233853\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.58065914133 0.380707126949\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.57979532335 0.383240534521\n",
      "most recent validation loss: 1.62871331741 0.364694444444\n",
      "training loss: 1.57554127857 0.380769766147\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n",
      "training loss: 1.5783354607 0.380651447661\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n",
      "training loss: 1.57709605924 0.381959910913\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n",
      "training loss: 1.57719721817 0.382126948775\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n",
      "training loss: 1.57799447001 0.382217427617\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n",
      "training loss: 1.57939806063 0.380477449889\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n",
      "training loss: 1.57645266909 0.382704621381\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n",
      "training loss: 1.57594475673 0.38239142539\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n",
      "training loss: 1.57310342545 0.38193903118\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n",
      "training loss: 1.57535644863 0.380895044543\n",
      "most recent validation loss: 1.63707904193 0.358722222222\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 5, 7])\n",
    "for tr, val in net.itertrain(train, test,  algo='sgd', learning_rate=1e-3, momentum=0.9, weight_l2=0.01):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['loss'], val['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.11      0.14       958\n",
      "          1       0.00      0.00      0.00       111\n",
      "          2       0.18      0.09      0.12      1024\n",
      "          3       0.42      0.75      0.54      1774\n",
      "          4       0.25      0.14      0.18      1247\n",
      "          5       0.49      0.51      0.50       831\n",
      "          6       0.34      0.38      0.36      1233\n",
      "\n",
      "avg / total       0.31      0.36      0.32      7178\n",
      "\n",
      "accuracy:  0.361939258846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natalia/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 200, 100 and 50 hidden nodes**\n",
    "* **'tanh' activation for hidden and 'relu' for input and output layers**\n",
    "* **Nesterovâ€™s accelerated gradient with learning rate 1e-3 and momentum 0.9**\n",
    "* **L2 regularization with parameter value 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8658271358 0.250069599109\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.81434032217 0.273141703786\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.75080122165 0.327637806236\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.71689344502 0.344619988864\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.69946302626 0.354363864143\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.68662195191 0.358818207127\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.67700879315 0.363446547884\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.66848317439 0.364643652561\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.65882961384 0.3695155902\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.65302436799 0.372369153675\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.64423973453 0.377032293987\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.63773997479 0.376893095768\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.63021577609 0.381451837416\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.62413021724 0.382405345212\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.61761379191 0.389490534521\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.61226824792 0.388738864143\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.60641217661 0.391488028953\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.59769114624 0.396603563474\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.59413395182 0.397738028953\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.58709070749 0.401823496659\n",
      "most recent validation loss: 0.377416666667\n",
      "training loss: 1.58091790764 0.406277839644\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.57461426162 0.408505011136\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.5694643334 0.409201002227\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.56170672373 0.412889755011\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.55776920224 0.416599387528\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.55082861305 0.419988864143\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.54535734841 0.422459632517\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.53830001746 0.426092706013\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.53314714515 0.428340757238\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.52885669235 0.428688752784\n",
      "most recent validation loss: 0.389638888889\n",
      "training loss: 1.52201775428 0.432864699332\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.51561018762 0.434173162584\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.50986915513 0.43727032294\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.50252873667 0.442051781737\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.49846222254 0.44481486637\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.49070461343 0.448503619154\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.48707526032 0.45091174833\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.47994011522 0.45388363029\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.47485933822 0.456437917595\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.46921932503 0.457447104677\n",
      "most recent validation loss: 0.403694444444\n",
      "training loss: 1.46282214533 0.462298162584\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.45635369504 0.463418708241\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.45135566067 0.465673719376\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.44546527684 0.469731347439\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.44233857545 0.471401726058\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.43464406027 0.475904788419\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.42771243053 0.477749164811\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.42111468943 0.481827672606\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.41628424342 0.485321547884\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.40973682099 0.486957126949\n",
      "most recent validation loss: 0.418138888889\n",
      "training loss: 1.40510794726 0.489128619154\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.39817542312 0.493360244989\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.39087621588 0.495636135857\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.38576666527 0.498559298441\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.37794603577 0.501760857461\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.37409378377 0.502874443207\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.3670857403 0.508268374165\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.35950180332 0.512597438753\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.35565177115 0.514344376392\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.34661850192 0.517330178174\n",
      "most recent validation loss: 0.418694444444\n",
      "training loss: 1.34155086799 0.520288140312\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.33609387713 0.523176503341\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.32812740242 0.527039253898\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.3216096485 0.531584075724\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.31561954078 0.536142817372\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.31081713844 0.535481625835\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.30041740336 0.540889476615\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.29429980051 0.543833518931\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.28733086104 0.543589922049\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.2821267515 0.551858296214\n",
      "most recent validation loss: 0.423694444444\n",
      "training loss: 1.274852784 0.555470489978\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.267814123 0.556166481069\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.25954783017 0.558811247216\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.25266682507 0.564761971047\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.24572847031 0.565430122494\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.24072200165 0.570211581292\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.23195355301 0.572348273942\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.22992458709 0.573357461024\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.21919029978 0.577345489978\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.21012274691 0.583832126949\n",
      "most recent validation loss: 0.423638888889\n",
      "training loss: 1.2022058479 0.588968541203\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.19495528535 0.589434855234\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.19212923032 0.589469654788\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.18323457715 0.594654788419\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.17085500271 0.602067093541\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.16881355106 0.604329064588\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.15860543442 0.609653396437\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.15075094046 0.609465478842\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.14339948511 0.612332962138\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.13518334638 0.616474109131\n",
      "most recent validation loss: 0.411888888889\n",
      "training loss: 1.12755472854 0.618005289532\n",
      "most recent validation loss: 0.428527777778\n",
      "training loss: 1.1185315017 0.627018374165\n",
      "most recent validation loss: 0.428527777778\n",
      "training loss: 1.10993945544 0.630080734967\n",
      "most recent validation loss: 0.428527777778\n",
      "training loss: 1.10915512694 0.631855512249\n",
      "most recent validation loss: 0.428527777778\n",
      "training loss: 1.09405051244 0.637701837416\n",
      "most recent validation loss: 0.428527777778\n",
      "training loss: 1.08724154517 0.639059020045\n",
      "most recent validation loss: 0.428527777778\n",
      "training loss: 1.07997013245 0.643548162584\n",
      "most recent validation loss: 0.428527777778\n",
      "training loss: 1.0771728574 0.646297327394\n",
      "most recent validation loss: 0.428527777778\n",
      "training loss: 1.06544721828 0.65061247216\n",
      "most recent validation loss: 0.428527777778\n",
      "training loss: 1.05849272303 0.651343262806\n",
      "most recent validation loss: 0.428527777778\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.16      0.21       958\n",
      "          1       0.00      0.00      0.00       111\n",
      "          2       0.27      0.22      0.24      1024\n",
      "          3       0.50      0.74      0.60      1774\n",
      "          4       0.33      0.29      0.31      1247\n",
      "          5       0.52      0.60      0.55       831\n",
      "          6       0.40      0.37      0.38      1233\n",
      "\n",
      "avg / total       0.39      0.42      0.39      7178\n",
      "\n",
      "accuracy:  0.418500975202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natalia/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], (200, 'tanh'), (100, 'tanh'), (50, 'tanh'), 7])\n",
    "for tr, val in net.itertrain(train, test,  algo='nag', learning_rate=1e-3, momentum=0.9, weight_l2=10):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['acc'])\n",
    "\n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 200, 100 and 50 hidden nodes**\n",
    "* **'softmax' for the output layer, 'relu' for others**\n",
    "* **Nesterovâ€™s accelerated gradient with learning rate 1e-3 and momentum 0.9**\n",
    "* **L2 regularization with parameter value 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.86933133307 0.252296770601\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.79927075865 0.291550668151\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.74158437984 0.333588530067\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.71143407406 0.347508351893\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.69107841798 0.356695434298\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.67678704532 0.361873608018\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.66521631609 0.367371937639\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.65315239337 0.372403953229\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.64149305201 0.374283129176\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.63265921387 0.379781458797\n",
      "most recent validation loss: 0.171861111111\n",
      "training loss: 1.62125989638 0.383414532294\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.61336705185 0.38521018931\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.60510109003 0.389365256125\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.59788041028 0.39352032294\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.59197299697 0.393332405345\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.58270445183 0.398921213808\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.57662742959 0.401426781737\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.5687799769 0.403298997773\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.56211633581 0.405428730512\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.55420534009 0.41341174833\n",
      "most recent validation loss: 0.36175\n",
      "training loss: 1.54793288804 0.413168151448\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.5404541493 0.418784799555\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.53535708312 0.420636135857\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.52770053747 0.424199610245\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.52204597138 0.427644766147\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.51402200217 0.431994710468\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.50899800353 0.435418986637\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.50100388542 0.436748329621\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.49735012789 0.438098552339\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.4882447625 0.439803730512\n",
      "most recent validation loss: 0.393361111111\n",
      "training loss: 1.48129079874 0.444800946548\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.47737831046 0.447320434298\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.47085467532 0.450173997773\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.46082708098 0.454976336303\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.45502290796 0.454697939866\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.45037055266 0.461880567929\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.44682128664 0.462527839644\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.43697201711 0.467622494432\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.43094693193 0.466773385301\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.42704945097 0.471220768374\n",
      "most recent validation loss: 0.406861111111\n",
      "training loss: 1.41972398168 0.475960467706\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.41307170314 0.475257516704\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.40596398018 0.482029510022\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.39917099624 0.481959910913\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.39357409777 0.486449053452\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.38755405327 0.492086581292\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.38014090248 0.491230512249\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.37414452505 0.495636135857\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.36868978816 0.498106904232\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.36181232611 0.501224944321\n",
      "most recent validation loss: 0.408805555556\n",
      "training loss: 1.35460261889 0.506166481069\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.34865818831 0.508177895323\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.3435897564 0.513606625835\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.33372501326 0.517246659243\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.32784321484 0.515833797327\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.32244287608 0.519821826281\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.31694863899 0.52424136971\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.30982144973 0.524366648107\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.30128371038 0.530519209354\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.29525608239 0.531284799555\n",
      "most recent validation loss: 0.410611111111\n",
      "training loss: 1.29053238277 0.537409521158\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.2826081066 0.537444320713\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.2732560364 0.543499443207\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.26437745518 0.54565701559\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.26083615609 0.546756681514\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.25397092607 0.551802616927\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.24456099276 0.555094654788\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.23919094967 0.557822939866\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.23317566642 0.562221603563\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.22591056691 0.563578786192\n",
      "most recent validation loss: 0.427722222222\n",
      "training loss: 1.21872665753 0.567462416481\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.20817150577 0.572522271715\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.20597703299 0.572960746102\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.19258859973 0.578633073497\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.1896011909 0.579656180401\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.18193659781 0.586720489978\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.17539406301 0.589608853007\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.1680005721 0.591453229399\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.15867188058 0.59253201559\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.15262761171 0.595420378619\n",
      "most recent validation loss: 0.417555555556\n",
      "training loss: 1.14873597476 0.595663975501\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.13925562895 0.602484688196\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.12924772615 0.60358435412\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.12111665881 0.610335467706\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.12039355457 0.608957405345\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.11068511602 0.614838530067\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.10001913069 0.617518095768\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.09885527871 0.617190979955\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.08609467813 0.625695991091\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.08302488642 0.626531180401\n",
      "most recent validation loss: 0.431722222222\n",
      "training loss: 1.07379980874 0.630171213808\n",
      "most recent validation loss: 0.429805555556\n",
      "training loss: 1.07000455958 0.633734688196\n",
      "most recent validation loss: 0.429805555556\n",
      "training loss: 1.05615373092 0.636783129176\n",
      "most recent validation loss: 0.429805555556\n",
      "training loss: 1.05318625605 0.637896714922\n",
      "most recent validation loss: 0.429805555556\n",
      "training loss: 1.04599872921 0.641181792873\n",
      "most recent validation loss: 0.429805555556\n",
      "training loss: 1.03565423406 0.646575723831\n",
      "most recent validation loss: 0.429805555556\n",
      "training loss: 1.02964690054 0.648211302895\n",
      "most recent validation loss: 0.429805555556\n",
      "training loss: 1.02448305354 0.648942093541\n",
      "most recent validation loss: 0.429805555556\n",
      "training loss: 1.01610219752 0.652630846325\n",
      "most recent validation loss: 0.429805555556\n",
      "training loss: 1.01075195171 0.656319599109\n",
      "most recent validation loss: 0.429805555556\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.25      0.26       958\n",
      "          1       0.00      0.00      0.00       111\n",
      "          2       0.27      0.17      0.21      1024\n",
      "          3       0.54      0.66      0.60      1774\n",
      "          4       0.37      0.22      0.28      1247\n",
      "          5       0.45      0.63      0.53       831\n",
      "          6       0.34      0.44      0.38      1233\n",
      "\n",
      "avg / total       0.39      0.41      0.39      7178\n",
      "\n",
      "accuracy:  0.409445528002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natalia/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 200, 100, 50, (7, 'softmax')])\n",
    "for tr, val in net.itertrain(train, test,  algo='nag', learning_rate=1e-3, momentum=0.9, weight_l2=10):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['acc'])\n",
    "\n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 200, 100 and 50 hidden nodes**\n",
    "* **'softmax' for the output layer, 'relu' for others**\n",
    "* **Layerwise approach**\n",
    "* **L2 regularization with parameter value 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.77279627833 0.285871380846\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.69082401545 0.331584075724\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.65918231099 0.34992344098\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.64073315451 0.359597717149\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.62772720458 0.368123608018\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.6151175173 0.371290367483\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.60379726759 0.374909521158\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.59223398805 0.379308184855\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.5809118913 0.385523385301\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.57153816514 0.389107739421\n",
      "most recent validation loss: 2.85657318862 0.0436111111111\n",
      "training loss: 1.56249586538 0.394849665924\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.54993857169 0.399095211581\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.54057614945 0.406069042316\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.5341544492 0.406695434298\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.52500586339 0.413049832962\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.51720987873 0.416696826281\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.5082666315 0.420531737194\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.49928576641 0.424547605791\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.49177616727 0.428083240535\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.48350679011 0.43200863029\n",
      "most recent validation loss: 1.59298740415 0.386611111111\n",
      "training loss: 1.47683714977 0.436087138085\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.47018766789 0.43543986637\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.46329376492 0.441752505568\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.45654970922 0.444487750557\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.44807272821 0.449290089087\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.44125778034 0.453180679287\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.43495342371 0.456667594655\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.4278950881 0.460565144766\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.42225105617 0.462966314031\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.41287012883 0.465103006682\n",
      "most recent validation loss: 1.56210767566 0.397555555556\n",
      "training loss: 1.40779896373 0.468457683742\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.40090932376 0.469397271715\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.39517734324 0.475055679287\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.38720014221 0.476886135857\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.38037695097 0.478152839644\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.37570239233 0.484743875278\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.36808493379 0.483337973274\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.36125294635 0.487597438753\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.35649352937 0.490388363029\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.34774208931 0.496540924276\n",
      "most recent validation loss: 1.52953897993 0.409638888889\n",
      "training loss: 1.34122149481 0.495434298441\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.33784403604 0.502909242762\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.33105555017 0.500633351893\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.3226243101 0.507384465479\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.31878992874 0.510321547884\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.31165383 0.513091592428\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.30550045063 0.51516564588\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.29775287415 0.517197939866\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.2916241299 0.51867344098\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.28728965441 0.522591870824\n",
      "most recent validation loss: 1.55077125822 0.410472222222\n",
      "training loss: 1.2807663781 0.526016146993\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.27421503364 0.529760579065\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.26800916211 0.531618875278\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.26265205431 0.534110523385\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.2566897217 0.537847995546\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.25036501687 0.542163140312\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.24431128716 0.541362750557\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.23850688673 0.544773106904\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.2326350908 0.547160356347\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.2278013578 0.55068903118\n",
      "most recent validation loss: 1.55938367332 0.409472222222\n",
      "training loss: 1.22117944075 0.55594376392\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.21572750163 0.553646993318\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.20987379078 0.557405345212\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.20479548388 0.558115256125\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.19787680134 0.563933741648\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.19472498836 0.566801224944\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.18671964991 0.570872772829\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.18256678785 0.570281180401\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.17873392208 0.574631124722\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.17053043339 0.576092706013\n",
      "most recent validation loss: 1.56201328752 0.422138888889\n",
      "training loss: 1.16517879454 0.577331570156\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.15892306592 0.580860244989\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.15356726992 0.585711302895\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.14844481822 0.587590478842\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.14348326069 0.584667316258\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.13712470762 0.590792037862\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.13333783655 0.594466870824\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.12676794287 0.594745267261\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.12091216544 0.599408407572\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.11818696144 0.597042037862\n",
      "most recent validation loss: 1.58383431741 0.423361111111\n",
      "training loss: 1.11123269899 0.603424276169\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.10831547544 0.604363864143\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.10109439817 0.606417037862\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.09487320464 0.610314587973\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.09097623629 0.611532572383\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.08770307096 0.610718262806\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.08183406517 0.613724944321\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.07395996268 0.618840478842\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.07107517461 0.620726614699\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.0677020289 0.621854120267\n",
      "most recent validation loss: 1.56380087739 0.428916666667\n",
      "training loss: 1.67945963406 0.346408685969\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.55975406774 0.401879175947\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.51402082714 0.422236915367\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.48405361404 0.432273106904\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.46442013161 0.442086581292\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.44854496375 0.450347995546\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.43364284508 0.450647271715\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.42202812998 0.458651169265\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.4098181703 0.465033407572\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.39876314831 0.469974944321\n",
      "most recent validation loss: 2.82788133952 0.133527777778\n",
      "training loss: 1.38904642893 0.474707683742\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.38056600448 0.477039253898\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.3698439102 0.481215200445\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.36193072675 0.483908685969\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.35199938279 0.49329064588\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.34400425509 0.494000556793\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.33553117305 0.496993318486\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.32582346753 0.501204064588\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.31895807315 0.505310412027\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.30905426838 0.512931514477\n",
      "most recent validation loss: 1.55833180061 0.397833333333\n",
      "training loss: 1.30047118696 0.512082405345\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.29306917048 0.515924276169\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.28300866908 0.522654510022\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.27614631727 0.523280902004\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.26722692885 0.528786191537\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.25950228869 0.52843123608\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.251066726 0.535968819599\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.24235069285 0.5378827951\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.23362304912 0.542900890869\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.22639517844 0.545691815145\n",
      "most recent validation loss: 1.56802320568 0.400722222222\n",
      "training loss: 1.21836944269 0.546875\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.2112708261 0.552916202673\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.20292063007 0.556131681514\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.19161668527 0.564170378619\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.18754874569 0.564003340757\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.17922248053 0.564574053452\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.17332129009 0.570002783964\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.16544388424 0.575375835189\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.15551044603 0.575222717149\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.14658447922 0.57941954343\n",
      "most recent validation loss: 1.5671480507 0.425333333333\n",
      "training loss: 1.13976446907 0.580839365256\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.13226163506 0.584375\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.12675771445 0.587854955457\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.11762433752 0.590200445434\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.10829402235 0.596290367483\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.1045858579 0.597807628062\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.09638514916 0.601078786192\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.08747517766 0.605059855234\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.08214550618 0.605895044543\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.07395181728 0.609291481069\n",
      "most recent validation loss: 1.62056818801 0.417166666667\n",
      "training loss: 1.06623854499 0.614212138085\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 1.05909695803 0.615764198218\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 1.05389257024 0.620267260579\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 1.0454739238 0.618986636971\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 1.03948778012 0.622758908686\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 1.02934015875 0.627261971047\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 1.02451666645 0.627964922049\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 1.01590087564 0.633282293987\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 1.01023306671 0.63429844098\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 1.00250125998 0.639755011136\n",
      "most recent validation loss: 1.67209474337 0.426194444444\n",
      "training loss: 0.995096484755 0.640416202673\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 0.989458663527 0.644905345212\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 0.982310050551 0.644800946548\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 0.973444388975 0.649206570156\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 0.968701500388 0.653243318486\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 0.962395698488 0.655380011136\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 0.955479167837 0.659695155902\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 0.943695786046 0.663105512249\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 0.940950258494 0.66227032294\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 0.937460322307 0.664671492205\n",
      "most recent validation loss: 1.69766707293 0.435611111111\n",
      "training loss: 1.66924551748 0.356639755011\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.53269994333 0.416070434298\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.48454258934 0.435801781737\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.45286654421 0.446958518931\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.43044930652 0.458108296214\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.40961329894 0.46684298441\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.39626411381 0.471680122494\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.38018774281 0.478619153675\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.3700205429 0.480985523385\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.35852576382 0.490040367483\n",
      "most recent validation loss: 3.35085345916 0.0154166666667\n",
      "training loss: 1.34908887864 0.489901169265\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.33703362468 0.498962973274\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.32811277592 0.501969654788\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.31807196015 0.504544821826\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.30776018034 0.509764755011\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.30035290418 0.511678730512\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.29080858231 0.516320991091\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.2829700875 0.521540924276\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.27451569294 0.523113864143\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.26556236772 0.529656180401\n",
      "most recent validation loss: 1.52876974925 0.413138888889\n",
      "training loss: 1.25601063962 0.534020044543\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.24528291886 0.537604398664\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.23738005633 0.540687639198\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.23170547242 0.541258351893\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.22480795908 0.545817093541\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.21285940507 0.553229398664\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.20653580166 0.554621380846\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.1962184183 0.558776447661\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.18946977692 0.56006403118\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.18137830298 0.564212138085\n",
      "most recent validation loss: 1.54717660816 0.421722222222\n",
      "training loss: 1.17070025827 0.569703507795\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.1644675036 0.573044265033\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.1557045819 0.574526726058\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.14835106443 0.579238585746\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.14003602547 0.58254454343\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.1302831455 0.588042873051\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.1215272663 0.589539253898\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.11348352619 0.593875278396\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.10650351729 0.596673162584\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.09810016807 0.601162305122\n",
      "most recent validation loss: 1.59807381996 0.427305555556\n",
      "training loss: 1.08963909311 0.602449888641\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.08147842808 0.606806792873\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.07345974398 0.610683463252\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.06851032681 0.612910634744\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.05674868971 0.617831291759\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.05185033753 0.617344097996\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.04428590428 0.62218123608\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.03418478524 0.621067650334\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.02978369712 0.627227171492\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.01984400758 0.63102032294\n",
      "most recent validation loss: 1.65549665152 0.404666666667\n",
      "training loss: 1.01395991567 0.634952672606\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 1.00641291269 0.636449053452\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 0.99579741606 0.640868596882\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 0.988743408246 0.645218541203\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 0.980653689896 0.645775334076\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 0.974057636495 0.65022967706\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 0.968399235796 0.653326837416\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 0.956804693744 0.657676781737\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 0.953582450777 0.66066954343\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 0.944652349359 0.662479120267\n",
      "most recent validation loss: 1.73833513491 0.4045\n",
      "training loss: 0.940103550143 0.66547188196\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "training loss: 0.926514046313 0.669647828508\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "training loss: 0.924113039535 0.671304287305\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "training loss: 0.917001383858 0.672083797327\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "training loss: 0.906274032572 0.676155345212\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "training loss: 0.903158638508 0.67706013363\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "training loss: 0.891932904161 0.683184855234\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "training loss: 0.885328697237 0.685725222717\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "training loss: 0.880779268934 0.687534799555\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "training loss: 0.872086158951 0.69153674833\n",
      "most recent validation loss: 1.7739994759 0.422027777778\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.12      0.19       958\n",
      "          1       0.00      0.00      0.00       111\n",
      "          2       0.28      0.23      0.25      1024\n",
      "          3       0.50      0.71      0.59      1774\n",
      "          4       0.30      0.41      0.34      1247\n",
      "          5       0.63      0.45      0.53       831\n",
      "          6       0.39      0.39      0.39      1233\n",
      "\n",
      "avg / total       0.41      0.41      0.39      7178\n",
      "\n",
      "accuracy:  0.413485650599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natalia/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 200, 100, 50, (7, 'softmax')])\n",
    "for tr, val in net.itertrain(train, test,  algo='layerwise', weight_l2=1):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['loss'], val['acc'])\n",
    "    \n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 200, 100 and 50 hidden nodes**\n",
    "* **'softmax' for the output layer, 'relu' for others**\n",
    "* **Layerwise approach**\n",
    "* **L2 regularization with parameter value 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.77839459152 0.286226336303\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.69557457951 0.339727171492\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.66641649682 0.351698218263\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.64907787315 0.36196408686\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.63453428916 0.370281180401\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.62249969081 0.374227449889\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.6129476283 0.377032293987\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.60314179755 0.383087416481\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.59593770988 0.385258908686\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.58497595573 0.389873329621\n",
      "most recent validation loss: 2.57113339411 0.174583333333\n",
      "training loss: 1.57732857578 0.395754454343\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.56878485584 0.396520044543\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.56094744748 0.401023106904\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.5540863719 0.403598273942\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.5474257317 0.40777422049\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.53987262534 0.411114977728\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.53378508366 0.415061247216\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.52827950031 0.416119153675\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.52112702087 0.419118875278\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.51510235564 0.424561525612\n",
      "most recent validation loss: 1.59340770501 0.389833333333\n",
      "training loss: 1.50919332792 0.426071826281\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.50270898467 0.42896714922\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.49685918003 0.434319320713\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.49207515676 0.435766982183\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.48724497479 0.437472160356\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.48212293601 0.44039532294\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.47661734922 0.446151169265\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.47140349911 0.445114142539\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.46447522144 0.449178730512\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.46153358654 0.451927895323\n",
      "most recent validation loss: 1.56613179319 0.402166666667\n",
      "training loss: 1.45653056419 0.453841870824\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.45082691203 0.457447104677\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.44580668767 0.460739142539\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.44116574378 0.464490534521\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.43735333315 0.462019766147\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.43050234573 0.467204899777\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.42759379282 0.467587694878\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.42193754046 0.474130011136\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.41775330576 0.473538418708\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.41382796989 0.477087973274\n",
      "most recent validation loss: 1.54942004071 0.407694444444\n",
      "training loss: 1.40761163222 0.479141146993\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.40403278474 0.483372772829\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.39948395083 0.485739142539\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.39588951372 0.487847995546\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.39056849278 0.488919821826\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.38746136064 0.489372216036\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.38202569732 0.494035356347\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.37783889147 0.495496937639\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.37458402222 0.495844933185\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.36990838932 0.499220489978\n",
      "most recent validation loss: 1.55535269014 0.402972222222\n",
      "training loss: 1.36681932708 0.500125278396\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.36285571177 0.501134465479\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.35849672645 0.504684020045\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.35433121447 0.509486358575\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.35038565471 0.511295935412\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.34656566378 0.510599944321\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.3436681111 0.514379175947\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.33823651781 0.514775890869\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.33399782648 0.517733853007\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.32889976328 0.521422605791\n",
      "most recent validation loss: 1.55727700198 0.418388888889\n",
      "training loss: 1.32740702397 0.523357461024\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.32222622091 0.52469376392\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.31839095824 0.528347717149\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.3150623181 0.533150055679\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.31192354339 0.531375278396\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.30772440283 0.534507238307\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.30360167618 0.534423719376\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.30142451672 0.5367344098\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.29746512933 0.53986636971\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.29172263671 0.542163140312\n",
      "most recent validation loss: 1.58638748859 0.414388888889\n",
      "training loss: 1.29089507748 0.543506403118\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.28483129249 0.547139476615\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.2822975235 0.548649777283\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.27746369786 0.548266982183\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.27460276934 0.5535077951\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.27226183644 0.550876948775\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.26714763038 0.554447383073\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.26420570165 0.555317371938\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.2614541583 0.555644487751\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.25615020081 0.558985244989\n",
      "most recent validation loss: 1.57410686762 0.429638888889\n",
      "training loss: 1.25288723844 0.561163697105\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.25033820159 0.562799276169\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.24759022628 0.564121659243\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.24467105237 0.567267538976\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.24166513901 0.567114420935\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.23696228778 0.569689587973\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.23379208047 0.573204342984\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.2317873767 0.574143930958\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.22895334888 0.576245824053\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.22467029686 0.579224665924\n",
      "most recent validation loss: 1.60555392304 0.420333333333\n",
      "training loss: 1.72050701865 0.33742344098\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.60016143622 0.392323218263\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.55585383145 0.411288975501\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.52729074574 0.421923719376\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.50815598583 0.429628340757\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.49203094981 0.439922048998\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.4797358806 0.444174554566\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.46725213332 0.450501113586\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.45728450662 0.455811525612\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.44919188467 0.458477171492\n",
      "most recent validation loss: 3.4007626246 0.171861111111\n",
      "training loss: 1.43983095382 0.461219376392\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.431619044 0.465750278396\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.4251317863 0.470566536748\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.41528739759 0.473712416481\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.40836686842 0.475751670379\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.399104623 0.480811525612\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.39417146029 0.480498329621\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.38777990659 0.482655902004\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.37982769034 0.488432628062\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.37330323624 0.493687360802\n",
      "most recent validation loss: 1.56092501539 0.412277777778\n",
      "training loss: 1.36563021255 0.496993318486\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.35935730572 0.498072104677\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.35377152414 0.498879454343\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.34705510448 0.504767538976\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.3401409504 0.508198775056\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.33424151589 0.510530345212\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.32664063691 0.513752783964\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.3213045363 0.517351057906\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.31338948429 0.517490256125\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.3070802423 0.522222995546\n",
      "most recent validation loss: 1.5882149863 0.404222222222\n",
      "training loss: 1.30045135734 0.528104120267\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.29440545997 0.529336024499\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.28693992229 0.533637249443\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.28087245091 0.530853285078\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.27563887406 0.537896714922\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.26836044975 0.539622772829\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.26258359973 0.542420657016\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.25561998464 0.546756681514\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.25049211038 0.549519766147\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.24368871673 0.552881403118\n",
      "most recent validation loss: 1.59006611604 0.417166666667\n",
      "training loss: 1.23795589337 0.555435690423\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.23214542495 0.561024498886\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.22585705417 0.555839365256\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.21904782178 0.562673997773\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.21194387892 0.568763919822\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.20477664702 0.569167594655\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.2007590077 0.570712694878\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.19279090296 0.57454064588\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.18895709128 0.57515311804\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.18110250593 0.582600222717\n",
      "most recent validation loss: 1.59638074643 0.423805555556\n",
      "training loss: 1.1762302195 0.581869432071\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.16826055727 0.585885300668\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.16340639953 0.590186525612\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.1586714773 0.591404510022\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.14962688103 0.592866091314\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.14512501547 0.594306792873\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.14209117148 0.596868040089\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.13398897166 0.598086024499\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.13103903127 0.602380289532\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.12117620929 0.607565423163\n",
      "most recent validation loss: 1.64200160901 0.417277777778\n",
      "training loss: 1.11764402136 0.608365812918\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.1118384166 0.611219376392\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.10858935259 0.609583797327\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.09955316865 0.615499721604\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.09340689122 0.616926503341\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.08850542357 0.622111636971\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.08556918593 0.620615256125\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.07884048782 0.624046492205\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.07329764225 0.628723552339\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.06596429242 0.632168708241\n",
      "most recent validation loss: 1.70898954914 0.409916666667\n",
      "training loss: 1.69792380157 0.355282572383\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.55630513239 0.417392817372\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.51273167869 0.435739142539\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.48899877826 0.443603841871\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.47059900317 0.451057906459\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.45701227753 0.457495824053\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.44501432019 0.461414253898\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.4338862616 0.469188474388\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.42366430281 0.474582405345\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.41523779076 0.477964922049\n",
      "most recent validation loss: 2.98635482653 0.245361111111\n",
      "training loss: 1.40598252695 0.479315144766\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.39656992138 0.482690701559\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.38966513945 0.486483853007\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.38045390237 0.490506681514\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.37288060531 0.498350501114\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.36601517045 0.499568485523\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.35634335592 0.500960467706\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.35184091505 0.508456291759\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.34252413664 0.510843541203\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.33549905854 0.512200723831\n",
      "most recent validation loss: 1.55981990526 0.416861111111\n",
      "training loss: 1.33063354247 0.517490256125\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.32134101299 0.519125835189\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.31384675327 0.523719376392\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.30945776153 0.524554565702\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.29994955597 0.527707405345\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.29397382047 0.535773942094\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.28967652903 0.535272828508\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.28001719838 0.540040367483\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.2717317883 0.540353563474\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.26637682696 0.545761414254\n",
      "most recent validation loss: 1.56548953637 0.417694444444\n",
      "training loss: 1.25938800557 0.548336581292\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.25294876804 0.554530902004\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.24607982738 0.55526169265\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.23882653329 0.557857739421\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.23333994428 0.560328507795\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.22431765454 0.562465200445\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.21964020781 0.566662026726\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.21062822366 0.571373886414\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.20626594281 0.576266703786\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.19641425358 0.575793429844\n",
      "most recent validation loss: 1.60214979419 0.423805555556\n",
      "training loss: 1.19127458662 0.580929844098\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.18623320759 0.580373051225\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.17983899539 0.584458518931\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.1704100293 0.588599665924\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.16613502868 0.589608853007\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.15596923716 0.594745267261\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.15125629067 0.594759187082\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.144437053 0.599074331849\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.1386949171 0.60160077951\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.13303032956 0.603514755011\n",
      "most recent validation loss: 1.65351184975 0.414527777778\n",
      "training loss: 1.12618258299 0.606382238307\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.11951684813 0.607426224944\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.11300806432 0.613272550111\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.10737457321 0.615256124722\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.1008783203 0.618040089087\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.09488045565 0.621520044543\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.08830198777 0.622390033408\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.08054298451 0.62621798441\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.07663504047 0.629245545657\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.06952697884 0.633755567929\n",
      "most recent validation loss: 1.67035071869 0.42225\n",
      "training loss: 1.06384515621 0.635300668151\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "training loss: 1.05662486823 0.635892260579\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "training loss: 1.05130089536 0.638919821826\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "training loss: 1.04600458409 0.643652561247\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "training loss: 1.03888356918 0.643791759465\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "training loss: 1.03181555538 0.646332126949\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "training loss: 1.02808368153 0.646192928731\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "training loss: 1.02125847617 0.650438474388\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "training loss: 1.01704269408 0.652456848552\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "training loss: 1.01038981893 0.656041202673\n",
      "most recent validation loss: 1.7676588932 0.417027777778\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.27      0.28       958\n",
      "          1       0.00      0.00      0.00       111\n",
      "          2       0.33      0.12      0.17      1024\n",
      "          3       0.59      0.61      0.60      1774\n",
      "          4       0.30      0.43      0.35      1247\n",
      "          5       0.59      0.52      0.55       831\n",
      "          6       0.37      0.46      0.41      1233\n",
      "\n",
      "avg / total       0.41      0.42      0.41      7178\n",
      "\n",
      "accuracy:  0.417525773196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natalia/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 200, 100, 50, (7, 'softmax')])\n",
    "for tr, val in net.itertrain(train, test,  algo='layerwise', weight_l2=10):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['loss'], val['acc'])\n",
    "    \n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 1000, 250 and 50 hidden nodes**\n",
    "* **'softmax' for the output layer, 'relu' for others**\n",
    "* **Nesterov's accelerated gradient with learning rate 1e-3 and momentum 0.9**\n",
    "* **L2 regularization with parameter value 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.84459338901 0.25883908686\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.7653915612 0.311108017817\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.71199688158 0.343019209354\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.68771165717 0.354746659243\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.66962689434 0.361845768374\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.65763816689 0.363864142539\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.64287321136 0.373329621381\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.63155052422 0.374735523385\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.62175409775 0.377902282851\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.61165967831 0.383052616927\n",
      "most recent validation loss: 0.247305555556\n",
      "training loss: 1.60321319791 0.385662583519\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.59259425002 0.38956013363\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.58302360864 0.392552895323\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.57284529333 0.400034799555\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.56493092836 0.40495545657\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.5538498899 0.408978285078\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.54673120407 0.410001391982\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.53588415152 0.418770879733\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.52997708794 0.417692093541\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.51864604652 0.424220489978\n",
      "most recent validation loss: 0.380916666667\n",
      "training loss: 1.51042764008 0.42583518931\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.50156544092 0.43239142539\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.49407832891 0.433853006682\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.48203030395 0.438516146993\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.47641043284 0.442385857461\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.46507107216 0.449213530067\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.45712199798 0.451364142539\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.45064498137 0.454642260579\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.44186059789 0.456542316258\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.42878288685 0.463063752784\n",
      "most recent validation loss: 0.381555555556\n",
      "training loss: 1.42444875229 0.46737889755\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.41427215927 0.474025612472\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.4039437754 0.476517260579\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.39727342545 0.48102032294\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.38830716248 0.484444599109\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.37725475275 0.490068207127\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.36956653536 0.492643374165\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.35965909029 0.496520044543\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.35112016975 0.501343262806\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.34124148566 0.507551503341\n",
      "most recent validation loss: 0.407555555556\n",
      "training loss: 1.33097560926 0.507676781737\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.32067734439 0.512200723831\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.30960486836 0.52012110245\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.30153484896 0.523962973274\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.29280734405 0.530345211581\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.27854768716 0.530477449889\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.27102165716 0.534764755011\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.25712802215 0.543325445434\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.24423947987 0.549958240535\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.23651393242 0.553173719376\n",
      "most recent validation loss: 0.419361111111\n",
      "training loss: 1.22395208313 0.558010857461\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.21829777812 0.557788140312\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.20522816967 0.564086859688\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.18877528445 0.574721603563\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.18050524137 0.576663418708\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.17027224442 0.576941815145\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.15714555191 0.589017260579\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.14310941094 0.594376391982\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.13435189226 0.594619988864\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.12250092991 0.598573218263\n",
      "most recent validation loss: 0.42675\n",
      "training loss: 1.1103400927 0.604329064588\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 1.09404195366 0.609305400891\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 1.08524717262 0.612506959911\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 1.07188746073 0.622828507795\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 1.05791273719 0.625904788419\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 1.04153710339 0.634848273942\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 1.02984953576 0.637569599109\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 1.0173033832 0.643200167038\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 1.00658568457 0.650668151448\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 0.987145535734 0.657119988864\n",
      "most recent validation loss: 0.420888888889\n",
      "training loss: 0.977410421054 0.660913140312\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.968868247028 0.664219097996\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.949948767301 0.673406180401\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.933979424082 0.680331291759\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.917824904093 0.687221603563\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.911182045111 0.687847995546\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.894914896064 0.692754732739\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.881703624657 0.698392260579\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.867057916608 0.704899777283\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.844608600787 0.712033685969\n",
      "most recent validation loss: 0.428194444444\n",
      "training loss: 0.840839961164 0.717114420935\n",
      "most recent validation loss: 0.418666666667\n",
      "training loss: 0.826471407329 0.719759187082\n",
      "most recent validation loss: 0.418666666667\n",
      "training loss: 0.810559374645 0.728319877506\n",
      "most recent validation loss: 0.418666666667\n",
      "training loss: 0.790854485479 0.735836581292\n",
      "most recent validation loss: 0.418666666667\n",
      "training loss: 0.782104475569 0.738168151448\n",
      "most recent validation loss: 0.418666666667\n",
      "training loss: 0.764809701487 0.746694042316\n",
      "most recent validation loss: 0.418666666667\n",
      "training loss: 0.752396368914 0.751531180401\n",
      "most recent validation loss: 0.418666666667\n",
      "training loss: 0.745273970345 0.753619153675\n",
      "most recent validation loss: 0.418666666667\n",
      "training loss: 0.726135662141 0.760961859688\n",
      "most recent validation loss: 0.418666666667\n",
      "training loss: 0.717356844171 0.762284242762\n",
      "most recent validation loss: 0.418666666667\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.23      0.26       958\n",
      "          1       0.00      0.00      0.00       111\n",
      "          2       0.26      0.22      0.24      1024\n",
      "          3       0.50      0.70      0.58      1774\n",
      "          4       0.33      0.25      0.28      1247\n",
      "          5       0.68      0.43      0.53       831\n",
      "          6       0.36      0.47      0.40      1233\n",
      "\n",
      "avg / total       0.40      0.41      0.39      7178\n",
      "\n",
      "accuracy:  0.408191696851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natalia/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 1000, 250, 50, (7, 'softmax')])\n",
    "for tr, val in net.itertrain(train, test,  algo='nag', learning_rate=1e-3, momentum=0.9, weight_l2=10):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['acc'])\n",
    "\n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's weight targets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6] [3995  436 4097 7215 4830 3171 4965]\n",
      "[0 1 2 3 4 5 6] [ 958  111 1024 1774 1247  831 1233]\n"
     ]
    }
   ],
   "source": [
    "u, c = np.unique(train[1], return_counts=True)\n",
    "ut, ct = np.unique(test[1], return_counts=True)\n",
    "print(u, c)\n",
    "print(ut, ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u, c = np.unique(train[1], return_counts=True)\n",
    "weights = np.array([train[1].shape[0]*1./c[x] for x in train[1]])\n",
    "weights_t = np.array([train[1].shape[0]*1./c[x] for x in test[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 200, 100 and 50 hidden nodes**\n",
    "* **'tanh' for the hidden layers, 'relu' for the input and output**\n",
    "* **Nesterov's accelerated gradient with learning rate 1e-3 and momentum 0.9**\n",
    "* **L2 regularization with parameter value 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.99102386583 0.191345945979\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.92002638327 0.249980615496\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.86887292881 0.276460659657\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.83978884411 0.295043774966\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.82180834187 0.304226029311\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.80648382134 0.31023872695\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.79168192801 0.318186327097\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.78259027435 0.322486429883\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.77246461955 0.321119894632\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.76702755022 0.328167451559\n",
      "most recent validation loss: 0.117444773536\n",
      "training loss: 1.75381163605 0.337485124805\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.74415735995 0.341514392348\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.73470133412 0.346580520152\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.73031626983 0.347777401305\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.71866392035 0.352948160358\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.71183775138 0.354931515684\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.70582317463 0.360504214539\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.69534176525 0.365289721103\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.68477986814 0.369503720861\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.6797226602 0.368521376188\n",
      "most recent validation loss: 0.321596052783\n",
      "training loss: 1.67079293668 0.374677140214\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.65965729475 0.380783895579\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.64984095819 0.384923374319\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.63928453921 0.39119048182\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.63050516778 0.393665414201\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.62340236452 0.392967446455\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.61202888432 0.403819741374\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.60499816793 0.405973080731\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.59397664584 0.40775259675\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.58556719047 0.415784746396\n",
      "most recent validation loss: 0.365506963825\n",
      "training loss: 1.57309281381 0.420971183279\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.56932675684 0.421572576012\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.55973155398 0.42349083564\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.5445893033 0.43119660955\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.53762528124 0.434868611393\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.53186141333 0.435542644588\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.52314310476 0.437844361618\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.51219007579 0.440863245616\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.50606464139 0.449361211915\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.49425222326 0.450393921343\n",
      "most recent validation loss: 0.386404207034\n",
      "training loss: 1.48177568679 0.45428151121\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.47836350453 0.456665125196\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.46442403711 0.463329577247\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.46358992024 0.46508822724\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.4449990962 0.472644163973\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.43919515039 0.472861672834\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.43191635495 0.47277937692\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.42361844057 0.477118789424\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.41710594338 0.486079331208\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.405406088 0.488123685643\n",
      "most recent validation loss: 0.394316414418\n",
      "training loss: 1.40146485723 0.48593323477\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.39624587295 0.491764527724\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.38354003215 0.493781767744\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.37336229813 0.500673568133\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.37060817686 0.499471734072\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.3522636778 0.509648533796\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.3501597144 0.51120508708\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.3469203387 0.51194705128\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.34134271025 0.513336046513\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.32910781075 0.519921987956\n",
      "most recent validation loss: 0.403985371618\n",
      "training loss: 1.32185365545 0.520978550818\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.30779714801 0.527314499741\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.29483743165 0.530781640386\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.29599279986 0.527982879026\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.29162612612 0.532910051078\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.27599533592 0.541459652778\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.28592087449 0.535346718331\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.26989555275 0.541866162659\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.27237653319 0.544290929086\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.25645701347 0.548115473995\n",
      "most recent validation loss: 0.390025992266\n",
      "training loss: 1.25430052181 0.550025266012\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.24762880979 0.554506329527\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.23196657374 0.561843053889\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.22772045664 0.563870990059\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.21832317993 0.566284174399\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.22096032153 0.566742442904\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.20253046462 0.571847716147\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.20321047285 0.576011822038\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.19619132497 0.577222217172\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.19764529077 0.57885350258\n",
      "most recent validation loss: 0.415381584543\n",
      "training loss: 1.1837350046 0.580117075054\n",
      "most recent validation loss: 0.414783176456\n",
      "training loss: 1.1703525881 0.588840897315\n",
      "most recent validation loss: 0.414783176456\n",
      "training loss: 1.16916070392 0.590680875454\n",
      "most recent validation loss: 0.414783176456\n",
      "training loss: 1.15761545846 0.596114917365\n",
      "most recent validation loss: 0.414783176456\n",
      "training loss: 1.1517220379 0.596514582732\n",
      "most recent validation loss: 0.414783176456\n",
      "training loss: 1.14780127791 0.602999218385\n",
      "most recent validation loss: 0.414783176456\n",
      "training loss: 1.13545975195 0.60344906641\n",
      "most recent validation loss: 0.414783176456\n",
      "training loss: 1.13313667496 0.604966535662\n",
      "most recent validation loss: 0.414783176456\n",
      "training loss: 1.12522328633 0.609963984761\n",
      "most recent validation loss: 0.414783176456\n",
      "training loss: 1.1234713645 0.609486974886\n",
      "most recent validation loss: 0.414783176456\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.17      0.21       958\n",
      "          1       0.09      0.59      0.16       111\n",
      "          2       0.21      0.09      0.13      1024\n",
      "          3       0.61      0.49      0.54      1774\n",
      "          4       0.31      0.37      0.34      1247\n",
      "          5       0.40      0.64      0.49       831\n",
      "          6       0.39      0.38      0.38      1233\n",
      "\n",
      "avg / total       0.39      0.37      0.36      7178\n",
      "\n",
      "accuracy:  0.369462245751\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], (200, 'tanh'), (100, 'tanh'), (50, 'tanh'), 7], weighted=True)\n",
    "for tr, val in net.itertrain((train[0], train[1], weights), (test[0], test[1], weights_t),  algo='nag', \n",
    "                             learning_rate=1e-3, momentum=0.9, weight_l2=10):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['acc'])\n",
    "\n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 1000, 250 and 50 hidden nodes**\n",
    "* **'softmax' for the output layer, 'relu' for others**\n",
    "* **Layerwise approach**\n",
    "* **L2 regularization with parameter value 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.91646476933 0.224323352239\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.83503834132 0.276980397861\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.79962639615 0.300427999005\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.78028529252 0.306975969479\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.76096825434 0.317595811155\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.74781308215 0.324908057338\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.73934419595 0.326597068123\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.72618898467 0.337434766587\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.71670461003 0.339661647911\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.70907239916 0.345434537275\n",
      "most recent validation loss: 2.54619926365 0.146367212137\n",
      "training loss: 1.69573079277 0.34729046658\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.68747129504 0.352248851075\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.68122886014 0.358855817513\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.66957791014 0.366304603257\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.659157558 0.365486907978\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.6500160945 0.37345621185\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.64581530983 0.374443092251\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.63624153404 0.377406047622\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.62901848774 0.379304540156\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.62082986002 0.385521748243\n",
      "most recent validation loss: 1.71044342866 0.350698274512\n",
      "training loss: 1.61590172977 0.387309509886\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.60790457347 0.395061709099\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.59690660873 0.398826301998\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.59540423569 0.396850099268\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.58551612248 0.404209751161\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.57865558016 0.405252942846\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.57313572014 0.40254328729\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.56419117878 0.41042962465\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.56217793071 0.412347542361\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.54857860054 0.418629001534\n",
      "most recent validation loss: 1.68690486359 0.354104029003\n",
      "training loss: 1.54807884143 0.421275174385\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.54158266993 0.423255063104\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.53563355548 0.424799908218\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.53081221112 0.426837607588\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.52263514264 0.434451742931\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.51721158107 0.433688079156\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.50971488065 0.438681745827\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.50560478476 0.439988262959\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.49941813335 0.443809173547\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.49577548708 0.442249782158\n",
      "most recent validation loss: 1.71965561582 0.348285772993\n",
      "training loss: 1.48942950307 0.447013791703\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.4843839237 0.451772281088\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.47922204485 0.451372192118\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.47356591172 0.452531586728\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.46915006059 0.457407929614\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.46409911002 0.460776945916\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.45598491929 0.464555268596\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.45162813914 0.463042125207\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.44767010585 0.46843963689\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.44274723535 0.4697206418\n",
      "most recent validation loss: 1.65504241751 0.383967557071\n",
      "training loss: 1.44052454763 0.471545541941\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.43092826945 0.473812538402\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.42958016433 0.475903323637\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.42721276286 0.476475956725\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.41724481635 0.481813992935\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.41464621551 0.482489391433\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.4120135596 0.484501174458\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.40644323057 0.489406147252\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.40520785144 0.484462822858\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.39587941075 0.493312455142\n",
      "most recent validation loss: 1.65837379402 0.378289273381\n",
      "training loss: 1.39259793981 0.492800568374\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.38609793876 0.501857724148\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.38363594578 0.49726072769\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.38193673315 0.49787626989\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.37605958804 0.496411267451\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.37375750262 0.501963832107\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.36741571693 0.507151893132\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.36357481446 0.511766389363\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.35707539627 0.512189580581\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.35780977911 0.510907824315\n",
      "most recent validation loss: 1.6682062095 0.396003320543\n",
      "training loss: 1.3527398172 0.511932142117\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.34751153559 0.517758948684\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.34350734166 0.517197211316\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.33979092521 0.520382184064\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.3385939609 0.520045511423\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.33121152401 0.525302830976\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.32776000288 0.527521079615\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.3252869683 0.528443638304\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.31904356336 0.531516149088\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.31990631359 0.531049483602\n",
      "most recent validation loss: 1.66557912333 0.378881872912\n",
      "training loss: 1.3117697313 0.532661938012\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.31043817049 0.535403574848\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.30716528842 0.538095275878\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.30334248529 0.540523773918\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.29955803793 0.540015415697\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.29835333866 0.542793308108\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.29193993155 0.546789360819\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.28869270053 0.547461404367\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.28341545716 0.551731312037\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.28213160647 0.552281808287\n",
      "most recent validation loss: 1.70753104024 0.393566465471\n",
      "training loss: 1.27641123259 0.551713744409\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.27660549395 0.55260116671\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.27255985888 0.555739131373\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.2660258097 0.557703038471\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.26404751947 0.562703286334\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.26180371113 0.560366752588\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.25935182766 0.559157075198\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.25434863284 0.566751478776\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.25308059436 0.564856857573\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.2476387125 0.569691271765\n",
      "most recent validation loss: 1.70059427552 0.409491534307\n",
      "training loss: 1.83228609914 0.291843272597\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.69402628186 0.361428815321\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.63466097499 0.387240949018\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.59237908626 0.400373352088\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.56580332365 0.408867587738\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.54098514775 0.416417940541\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.52250282878 0.430154437707\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.50888272362 0.433175516584\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.49565551699 0.437777867609\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.48591745387 0.441736120159\n",
      "most recent validation loss: 2.78461794844 0.146587437292\n",
      "training loss: 1.47430569935 0.45125889717\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.462481916 0.449058105035\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.45631895557 0.455020115536\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.44251822403 0.460446510799\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.43665494111 0.461133699318\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.42790950564 0.466233497519\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.41567086627 0.473922835222\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.41147969652 0.470660222639\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.40157151012 0.475917312262\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.3985012093 0.476393659743\n",
      "most recent validation loss: 1.63646691565 0.382770012732\n",
      "training loss: 1.39078928665 0.481927546136\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.38290127019 0.487047209551\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.37207143714 0.491886957829\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.36409685655 0.491283908452\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.35862564922 0.497433374869\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.35313462789 0.496589209186\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.34381086049 0.502068851106\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.33593337531 0.503995670339\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.32919410821 0.51111990144\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.32487045743 0.51126642837\n",
      "most recent validation loss: 1.67049959034 0.392304762652\n",
      "training loss: 1.31721044171 0.515691937443\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.31123404302 0.516722552597\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.30078707776 0.52015905366\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.29616336321 0.520135427764\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.28893124241 0.523152988904\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.28643900611 0.526352182037\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.27599444068 0.533727017333\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.27353553413 0.532266790146\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.2641153307 0.535292659005\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.25775811071 0.537263776674\n",
      "most recent validation loss: 1.83820345004 0.366496791605\n",
      "training loss: 1.24973396145 0.544389786267\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.24325037362 0.547159312534\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.23562179115 0.54530561153\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.23370803287 0.549759448529\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.22879847373 0.552392664306\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.22092159762 0.554950589366\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.21343795428 0.559310129091\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.20703240017 0.559926146039\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.20057498071 0.56128919502\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.19565859317 0.567699025591\n",
      "most recent validation loss: 1.73207386591 0.38669973923\n",
      "training loss: 1.18796943623 0.571255948547\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.18698193557 0.569528166254\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.17926041541 0.573413951162\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.17030603942 0.576001343346\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.16660847272 0.581285228815\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.15751251841 0.582855032271\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.15242527399 0.586516574914\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.15360257349 0.583041982261\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.14534768266 0.586950493273\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.13884628412 0.593947635931\n",
      "most recent validation loss: 1.83936450359 0.393216326462\n",
      "training loss: 1.13243871191 0.593476807865\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.12423318682 0.599387314202\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.11729105245 0.600397861297\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.11188424124 0.60224224223\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.10821891774 0.604662077606\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.10162579242 0.60931020394\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.09975270157 0.609007203806\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.09291086229 0.613359753268\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.08917492899 0.612671282211\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.08492889101 0.615066744253\n",
      "most recent validation loss: 1.84675368544 0.39568077353\n",
      "training loss: 1.79227647362 0.314574544204\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.6278502234 0.393571406989\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.56535837559 0.41783632565\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.53293859094 0.429087588333\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.50980883665 0.438878531598\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.49302077638 0.441620586677\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.47827757667 0.451395289686\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.46601879535 0.456078428274\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.45111292339 0.458151173017\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.44582176174 0.461237963536\n",
      "most recent validation loss: 2.72495428682 0.147645406925\n",
      "training loss: 1.4365702223 0.466658575118\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.42268313327 0.476860295319\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.41487691354 0.473082620214\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.40966814313 0.476229184006\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.39934807061 0.478311977082\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.3934079107 0.484401926195\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.37979753995 0.488297757437\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.37813128189 0.493806795578\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.36656327725 0.492266602729\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.36032642547 0.497384942812\n",
      "most recent validation loss: 1.76974566295 0.36387308594\n",
      "training loss: 1.35057827921 0.503082926109\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.33956452859 0.50867004756\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.33556094195 0.510925895922\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.32629411244 0.509198232112\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.32437922959 0.512637723911\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.31375739114 0.517836723309\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.30843852996 0.516281371953\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.30077835007 0.520680241409\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.29583609027 0.526169878521\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.28840470874 0.52721067903\n",
      "most recent validation loss: 1.70906699151 0.390731028146\n",
      "training loss: 1.27881932602 0.532409544125\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.27283573579 0.533417268227\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.27252491366 0.53506244224\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.26125350813 0.537158629767\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.25598839479 0.539568275302\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.25052745085 0.542006465935\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.2419905057 0.547775731681\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.235969181 0.550212195883\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.22569104392 0.552555362611\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.22150687341 0.556654563204\n",
      "most recent validation loss: 1.68491184609 0.41094067219\n",
      "training loss: 1.21484134542 0.559223141039\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.21096493207 0.560548127543\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.20604021939 0.564161498033\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.19875767173 0.56602693503\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.18824963239 0.569600103782\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.18370530719 0.572848279067\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.18357049368 0.573309977292\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.17582972673 0.575936187073\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.16859971528 0.579185923664\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.15759735858 0.58190266796\n",
      "most recent validation loss: 1.7219890516 0.404041434903\n",
      "training loss: 1.15491474948 0.584183493224\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.14862415126 0.589066983934\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.14275209917 0.590136355124\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.13970998134 0.592123385266\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.13105059065 0.594692215661\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.12512898345 0.597624575397\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.11824788775 0.601510983856\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.11381539459 0.600239053576\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.10915570585 0.604065441295\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.10604711427 0.607214716841\n",
      "most recent validation loss: 1.91497059441 0.396423850975\n",
      "training loss: 1.09883499259 0.61050109485\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.08827441194 0.613744056693\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.08699881304 0.613358102171\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.08233905606 0.616047071913\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.07853233307 0.617310231306\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.0699342745 0.621984962821\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.06356928973 0.624064582041\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.05980472741 0.625525980375\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.05277354273 0.629870434794\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.05100282796 0.626917554393\n",
      "most recent validation loss: 1.93425224327 0.407360394335\n",
      "training loss: 1.03968499021 0.63284047737\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 1.03845297831 0.634678658927\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 1.03452568403 0.637051184306\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 1.02970186744 0.63899531347\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 1.02198120774 0.641287157098\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 1.01921331481 0.644362413769\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 1.0113665274 0.645061933165\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 1.00968506123 0.643988558165\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 1.0020437385 0.654094688376\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 0.995934043726 0.652142630171\n",
      "most recent validation loss: 1.95271053131 0.408307491517\n",
      "training loss: 0.99206432933 0.656308848651\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "training loss: 0.988646880724 0.653595107592\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "training loss: 0.984539114962 0.658369311761\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "training loss: 0.975170035145 0.662271068356\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "training loss: 0.975101507114 0.660724491185\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "training loss: 0.96258931608 0.664980533617\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "training loss: 0.963018957883 0.66775763457\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "training loss: 0.957366081617 0.667863709903\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "training loss: 0.948405308147 0.674393834726\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "training loss: 0.946338252943 0.671246440137\n",
      "most recent validation loss: 1.98974993256 0.399530516988\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.33      0.29       958\n",
      "          1       0.18      0.55      0.27       111\n",
      "          2       0.29      0.18      0.22      1024\n",
      "          3       0.62      0.52      0.57      1774\n",
      "          4       0.31      0.32      0.32      1247\n",
      "          5       0.44      0.68      0.53       831\n",
      "          6       0.42      0.32      0.36      1233\n",
      "\n",
      "avg / total       0.41      0.40      0.39      7178\n",
      "\n",
      "accuracy:  0.395792699916\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 200, 100, 50, (7, 'softmax')], weighted=True)\n",
    "for tr, val in net.itertrain((train[0], train[1], weights), (test[0], test[1], weights_t),  algo='layerwise', \n",
    "                             weight_l2=10):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['loss'], val['acc'])\n",
    "    \n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 1000, 250 and 50 hidden nodes**\n",
    "* **'softmax' for the output layer, 'relu' for others**\n",
    "* **Layerwise approach**\n",
    "* **L2 regularization with parameter value 100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.04735880752 0.221241982899\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.92810543747 0.267169926265\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.88106651261 0.295491551373\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.85295961043 0.300917734557\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.83187498757 0.309532530529\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.81521813402 0.317185188007\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.80242488972 0.320775467745\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.78751175823 0.323919203643\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.77660725999 0.33230631223\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.77003665165 0.333692169089\n",
      "most recent validation loss: 2.86184094797 0.141205496115\n",
      "training loss: 1.76256505578 0.334078936411\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.75347166681 0.34164938473\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.74495126354 0.346657505245\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.73936241457 0.350069926157\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.72981872999 0.35403440447\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.72658992631 0.352727670063\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.7226617126 0.359024438968\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.7155315431 0.359829234257\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.71156942025 0.361695240248\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.7059573941 0.367968537847\n",
      "most recent validation loss: 1.763622442 0.331168074482\n",
      "training loss: 1.69990520594 0.369862885944\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.6970257176 0.369607957908\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.69125599852 0.374670453425\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.68641374434 0.379057059133\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.68208104616 0.381887966098\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.67964335026 0.381735421304\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.67190593684 0.384457723175\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.67125511605 0.386235794142\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.66487397457 0.386542076475\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.66386197625 0.386435515745\n",
      "most recent validation loss: 1.71812446624 0.370790069581\n",
      "training loss: 1.6589663888 0.390570225467\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.65446505318 0.395729613629\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.64968142458 0.395715773818\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.64664003702 0.402514415252\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.64420753787 0.40012318993\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.64201155622 0.404167189148\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.63883762856 0.40565325541\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.63705936567 0.404034161746\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.63139408182 0.404208864924\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.63016494255 0.409808999918\n",
      "most recent validation loss: 1.70788694019 0.375061141862\n",
      "training loss: 1.62619626225 0.417198857634\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.62189967286 0.417314460932\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.62106941818 0.417961120453\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.61847630819 0.421214757679\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.61725653839 0.418528803343\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.61321810962 0.41993096705\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.61080793078 0.423595991698\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.60851691409 0.425361925608\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.60555798372 0.42600374874\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.60239962955 0.429912012936\n",
      "most recent validation loss: 1.78124667033 0.337746276853\n",
      "training loss: 1.59853219739 0.42946415084\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.59898345395 0.429270246619\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.5945678657 0.435790808455\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.5930851069 0.433984101661\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.58989391588 0.432177480432\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.586823692 0.437292412425\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.58845738798 0.436117184134\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.58481589443 0.4373571788\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.58211695903 0.440583728118\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.58190992466 0.438546898712\n",
      "most recent validation loss: 1.73172348724 0.370577193788\n",
      "training loss: 1.57874332373 0.444056467284\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.57594816868 0.445075759382\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.5738132406 0.446572061306\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.57266478203 0.447306676812\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.5686067105 0.450566458599\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.56734260455 0.446771994401\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.56742400957 0.448637306041\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.56618904988 0.451251014662\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.56229012115 0.452838737347\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.56213987059 0.453495229582\n",
      "most recent validation loss: 1.70628462078 0.385398048314\n",
      "training loss: 1.56177787526 0.452664441204\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.55837267622 0.451199526658\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.55664786174 0.457614479778\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.55646402222 0.456922068438\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.55312828396 0.459964756985\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.55322454482 0.45920795293\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.55097164903 0.463376343817\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.55050863765 0.458838968869\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.54436914272 0.466591150491\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.54414321315 0.467741258932\n",
      "most recent validation loss: 1.70811044666 0.39303519825\n",
      "training loss: 1.54095769484 0.465858269455\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.54161418857 0.466830315446\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.54069984313 0.467957324217\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.53854447559 0.47401502793\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.53800720519 0.468351101366\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.53843597086 0.47049954415\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.53447823924 0.475814082515\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.53288468473 0.471422051618\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.53286484999 0.47609456858\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.53005527079 0.477062019712\n",
      "most recent validation loss: 1.71561291255 0.392642902943\n",
      "training loss: 1.53010449986 0.475870078866\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.52776015926 0.477739332712\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.52503086639 0.480348677849\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.52642920268 0.478965249526\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.52448291779 0.477850179502\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.52146749655 0.482064526645\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.5216188377 0.483663621618\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.51874737355 0.481548662305\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.52142589542 0.482912426063\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.51612690981 0.485356139511\n",
      "most recent validation loss: 1.71723620115 0.391342201625\n",
      "training loss: 1.51704007013 0.486441335168\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.51440708181 0.487527021809\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.51352670978 0.487529969963\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.5131303215 0.489122665724\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.51231351652 0.488308073721\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.51184243835 0.490350614412\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.50962134442 0.491588734499\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.50626151363 0.492312832291\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.5080858225 0.493824800566\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.50513052889 0.4964645597\n",
      "most recent validation loss: 1.72043083361 0.394537734047\n",
      "training loss: 1.50528022387 0.494073645331\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.5035085484 0.491533923279\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.50256586205 0.494027049162\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.5020011711 0.495385952655\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.50077728545 0.498894651151\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.49794953712 0.49994345235\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.49876665779 0.500175078591\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.49895896621 0.499497523225\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.49673185167 0.503543460314\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.49556693188 0.502944539599\n",
      "most recent validation loss: 1.75700973888 0.395161080091\n",
      "training loss: 1.99470135726 0.326310819642\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.79306997955 0.396588661979\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.73166618729 0.414739901331\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.68925886713 0.429772782382\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.66133748564 0.431256716882\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.63769508544 0.439857177917\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.62473769837 0.438298781856\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.60677998969 0.447328155698\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.59649633761 0.446850882353\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.58089494128 0.455006197794\n",
      "most recent validation loss: 3.21806954879 0.146922535165\n",
      "training loss: 1.57121805712 0.45760857955\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.56858101605 0.455799432184\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.56214889509 0.458270334769\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.55161628067 0.461007279743\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.54455413134 0.463109470178\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.54114456864 0.464151709363\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.5330183339 0.468305046076\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.52859994486 0.468963138211\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.52366517659 0.469704196105\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.52134575071 0.471729250517\n",
      "most recent validation loss: 1.76574817242 0.387519089662\n",
      "training loss: 1.51406409427 0.47389458475\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.51069571047 0.476881618003\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.50394492203 0.478190654239\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.50458734927 0.477352823874\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.49591846386 0.482351667046\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.49406299395 0.48455985385\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.48915997083 0.482320066422\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.48684303395 0.486114823839\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.48461169418 0.488653071476\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.48227574132 0.490879228688\n",
      "most recent validation loss: 1.74815228378 0.393696161078\n",
      "training loss: 1.47660800481 0.49136616723\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.47460513784 0.493103526814\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.47178399989 0.493855741228\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.46904224268 0.496879577861\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.46926401661 0.494711625013\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.46192825248 0.498396882719\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.46188085097 0.500007380831\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.45862095184 0.50125600861\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.45344181779 0.504061992498\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.45211227871 0.503322226736\n",
      "most recent validation loss: 1.7198580406 0.405479036935\n",
      "training loss: 1.45262858261 0.504243214191\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.44805871471 0.50553725009\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.44646206874 0.509333852668\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.4424448013 0.509183851193\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.44144791785 0.508689954584\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.43480534406 0.511718870833\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.43517649684 0.510421177673\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.43114119191 0.516388585236\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.42953882218 0.515789315897\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.42791771692 0.516966156523\n",
      "most recent validation loss: 1.74577041429 0.409634603256\n",
      "training loss: 1.42554633625 0.517552873621\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.42294818843 0.515994337397\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.42027211944 0.521182301131\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.41710467766 0.522525748015\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.41529131279 0.523467402217\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.41576881564 0.522694096935\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.40767768214 0.524982623766\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.40930271236 0.526372355712\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.40479258291 0.529698224902\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.40367444124 0.530100257635\n",
      "most recent validation loss: 1.73781273343 0.414589926312\n",
      "training loss: 1.4006991815 0.53327386986\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.40245807289 0.53062031431\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.39733843605 0.534740848553\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.39565381388 0.533526930042\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.39366029044 0.535487971205\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.39058886461 0.535478456215\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.39049951771 0.538594997867\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.3893385939 0.533129900861\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.38710993334 0.539571153023\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.3822562268 0.541366139277\n",
      "most recent validation loss: 1.74637474443 0.416414896968\n",
      "training loss: 1.37989849889 0.540090355091\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.38011319107 0.543931857218\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.37831788328 0.546383663368\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.37550804664 0.545878566982\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.37353677688 0.548527918172\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.37318757199 0.548543007163\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.36953289202 0.551277118471\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.36877895438 0.550359915708\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.36690579024 0.550355235206\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.3641885399 0.551523708671\n",
      "most recent validation loss: 1.81231635108 0.407140509831\n",
      "training loss: 1.36058798699 0.553474978429\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 1.35938938014 0.555637612654\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 1.35768062628 0.556045216969\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 1.3566977699 0.556424812927\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 1.3561656821 0.558784582285\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 1.35641623908 0.55719341918\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 1.35084528183 0.560058618452\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 1.35100612829 0.561552180035\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 1.34900590383 0.562554864378\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 1.34608083228 0.562965934469\n",
      "most recent validation loss: 1.80332666733 0.413802482062\n",
      "training loss: 2.0012520788 0.389042277671\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.72785758824 0.458085787852\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.6697523394 0.475899376582\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.63450032876 0.483457617629\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.61084571352 0.487939660952\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.59521230944 0.486705493692\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.57739405775 0.490359758762\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.56221907183 0.497934062005\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.55185605861 0.496870153282\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.54424751696 0.495708249235\n",
      "most recent validation loss: 3.94116375939 0.147142222628\n",
      "training loss: 1.52948239881 0.501912442097\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.52607943502 0.500755454856\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.51587606378 0.503995837294\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.5088768299 0.503128729841\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.50238210065 0.503383796581\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.49536734405 0.51104623079\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.49013939522 0.508606753558\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.48340128288 0.511858405898\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.47631767076 0.514089048049\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.47550377528 0.513506186682\n",
      "most recent validation loss: 1.84216771813 0.403134353319\n",
      "training loss: 1.46926102988 0.515302370618\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.46581640815 0.51634203717\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.46336014871 0.517891959581\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.45588079145 0.519923354768\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.45632147366 0.519961349047\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.45159244333 0.521304727255\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.44812006756 0.519245054265\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.44507770182 0.526424075913\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.44389833413 0.524888698889\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.43848597552 0.526771428871\n",
      "most recent validation loss: 1.85351574095 0.399292815696\n",
      "training loss: 1.43797184563 0.52569250201\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.43434558037 0.528600606376\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.42933995343 0.53024949564\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.42613858912 0.531551219179\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.42471206216 0.534275305881\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.4236960105 0.531531718738\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.41888212998 0.534892235593\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.413849457 0.53657560685\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.41178076672 0.538994518155\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.41101443563 0.539797878557\n",
      "most recent validation loss: 1.86863538751 0.403710470432\n",
      "training loss: 1.40645082248 0.540660128684\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.40300727847 0.543145334725\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.40268738202 0.543740351317\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.39814488301 0.54539407419\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.39642444575 0.545046574116\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.39201924492 0.548348163803\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.39100688978 0.548670654048\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.39109355139 0.550883619104\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.38862334881 0.549378073448\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.38564259091 0.548355974078\n",
      "most recent validation loss: 1.83438224854 0.40520019653\n",
      "training loss: 1.38400758178 0.55147649541\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.38309602926 0.553502628649\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.37907860306 0.555475951772\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.37654032297 0.556475523618\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.37275299606 0.560701116178\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.37078016398 0.558588915449\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.36884161878 0.561953857332\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.36895494712 0.562235942676\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.36353777421 0.561336031034\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.36172130013 0.563161155562\n",
      "most recent validation loss: 1.8367296565 0.410469092794\n",
      "training loss: 1.36121863151 0.560112665778\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.35861487724 0.565029525545\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.35753676229 0.567391395035\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.3543088888 0.568063952893\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.35342363311 0.56676337115\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.34956214402 0.567880957389\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.34613848215 0.572583186086\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.34780593729 0.570758837025\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.34656647913 0.573675362791\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.34213160796 0.575733171832\n",
      "most recent validation loss: 1.85626054375 0.407399651869\n",
      "training loss: 1.34094305025 0.573913397809\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.33884615341 0.578174155851\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.33644702427 0.579332341632\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.33564316016 0.579187002797\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.33243928248 0.577715924292\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.33259230112 0.58019606211\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.32892723647 0.579949401772\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.32646079939 0.582219089743\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.32412447729 0.585607164158\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.32183788605 0.588023369505\n",
      "most recent validation loss: 1.90742117735 0.411684775314\n",
      "training loss: 1.32141799772 0.587345089014\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.31817722266 0.588852488029\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.3146721096 0.588727846544\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.31600004423 0.589654766868\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.31528706682 0.590079344612\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.31182849579 0.590985119323\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.30903404454 0.593960763378\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.30700603527 0.597075640519\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.30429082209 0.594452043349\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.30460914627 0.597051187625\n",
      "most recent validation loss: 1.91194270803 0.410191504106\n",
      "training loss: 1.30213668248 0.598929422135\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "training loss: 1.30174518046 0.598052961163\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "training loss: 1.29913945876 0.598925057523\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "training loss: 1.29638883471 0.600643385487\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "training loss: 1.29473355978 0.601491962959\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "training loss: 1.29186489952 0.604745327461\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "training loss: 1.29191991757 0.604477085894\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "training loss: 1.28840883653 0.608959713057\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "training loss: 1.28665541979 0.607189225035\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "training loss: 1.28503250072 0.609011625813\n",
      "most recent validation loss: 1.88406140611 0.405293878403\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.31      0.22      0.26       958\n",
      "          1       0.27      0.49      0.35       111\n",
      "          2       0.26      0.34      0.29      1024\n",
      "          3       0.62      0.56      0.58      1774\n",
      "          4       0.32      0.27      0.29      1247\n",
      "          5       0.57      0.55      0.56       831\n",
      "          6       0.36      0.44      0.39      1233\n",
      "\n",
      "avg / total       0.42      0.41      0.41      7178\n",
      "\n",
      "accuracy:  0.408052382279\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 200, 100, 50, (7, 'softmax')], weighted=True)\n",
    "for tr, val in net.itertrain((train[0], train[1], weights), (test[0], test[1], weights_t),  algo='layerwise', \n",
    "                             weight_l2=100):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['loss'], val['acc'])\n",
    "    \n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 1000, 250 and 50 hidden nodes**\n",
    "* **'softmax' for the output layer, 'relu' for others**\n",
    "* **Layerwise approach**\n",
    "* **L1 regularization with parameter value 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1113664769 0.221067271518\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.98238444953 0.272448902999\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.93062408773 0.293412909419\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.90269642563 0.305434751427\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.88013212816 0.308884915246\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.86534229642 0.319892904891\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.85064587745 0.318419154344\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.84162259042 0.324824425615\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.83066477176 0.32819526246\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.81786718364 0.332060251961\n",
      "most recent validation loss: 3.29024745042 0.142393157926\n",
      "training loss: 1.8128126505 0.335491707596\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.80132459283 0.342094027232\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.79760105665 0.343372573706\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.79076522928 0.344361618859\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.78294312094 0.351256299431\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.77891156623 0.348588996643\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.77145667903 0.356838407924\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.76775818176 0.350436610834\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.7580881858 0.365391818674\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.75304053509 0.360164862595\n",
      "most recent validation loss: 1.84078777371 0.315475659246\n",
      "training loss: 1.75071474856 0.361002200561\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.74808910214 0.362487222748\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.73978542501 0.36934346813\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.73508268899 0.371418454742\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.7317158039 0.370382495331\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.72679830881 0.377688207066\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.72388643888 0.378865322018\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.71808509435 0.380396718656\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.71490059058 0.382134939053\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.70871587095 0.382031463502\n",
      "most recent validation loss: 1.7700231881 0.355542602154\n",
      "training loss: 1.70665212435 0.389541371183\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.7009755431 0.385978443767\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.69737447816 0.387979814897\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.69415010539 0.391509780757\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.68999744798 0.393649512452\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.68544235933 0.396599669772\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.68231008991 0.396883658146\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.67773558237 0.40222753336\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.67473689391 0.39899016953\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.67136110546 0.400325575171\n",
      "most recent validation loss: 1.75964416838 0.360900737787\n",
      "training loss: 1.66580098731 0.402124002901\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.6638354063 0.407015833139\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.65934103731 0.411012008941\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.6583227066 0.414249656654\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.65331588946 0.413142865956\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.64904000886 0.413802220594\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.64684452881 0.41534417587\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.64261216101 0.416430024114\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.63946187237 0.419649708237\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.63613793561 0.421961140765\n",
      "most recent validation loss: 1.75145145995 0.363910261548\n",
      "training loss: 1.63137383161 0.425547314372\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.62982103628 0.426172526631\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.62465158307 0.431237796133\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.62214036252 0.428752035285\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.61826547301 0.430327818303\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.61775365126 0.425859699149\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.61425955795 0.43054017059\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.61154315552 0.438008055276\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.60745549458 0.439659121128\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.60357172841 0.439936499494\n",
      "most recent validation loss: 1.75586976445 0.359978247641\n",
      "training loss: 1.60196948301 0.440400479244\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.59674066666 0.43973001915\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.59668950319 0.44386064676\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.59415138561 0.446484616112\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.59180771987 0.445442330243\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.58518819581 0.447337889319\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.58565545336 0.448527150673\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.58418873913 0.449940436712\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.58021870645 0.451254053261\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.57626098752 0.451170825565\n",
      "most recent validation loss: 1.79876347864 0.366014927896\n",
      "training loss: 1.57566220392 0.455022703863\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.57282659111 0.456363547728\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.57070520024 0.458941444411\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.56975388953 0.455328537339\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.56822155957 0.462059507285\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.5630933314 0.461629722831\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.55964787881 0.462046336441\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.55987611901 0.459883864216\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.55320820284 0.467019421229\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.55387139267 0.463447834237\n",
      "most recent validation loss: 1.79975548269 0.362400647372\n",
      "training loss: 1.54912283892 0.466200078817\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.54912675094 0.468096418656\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.54542551223 0.468770648077\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.54243721598 0.470217483274\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.54288873765 0.473068576451\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.54034772527 0.474795475276\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.5361663205 0.471487258403\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.53553223788 0.47166184959\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.53312126138 0.47632557124\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.53235555338 0.475582800544\n",
      "most recent validation loss: 1.7611803985 0.379434718787\n",
      "training loss: 1.52677213371 0.477326954235\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 1.52636586293 0.478315194287\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 1.52437254262 0.479430430795\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 1.52277017379 0.48248412612\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 1.52074679928 0.483121860137\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 1.51692693492 0.481663438459\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 1.51643076733 0.484548053226\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 1.51360694173 0.487395221008\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 1.51190109731 0.486740789621\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 1.50996802049 0.48554440543\n",
      "most recent validation loss: 1.75355280656 0.393730547324\n",
      "training loss: 2.09072095967 0.274405078893\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.89929230145 0.331165602883\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.84618448363 0.34889901447\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.81029520585 0.362993598772\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.784717315 0.372629222503\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.76613341923 0.383054280815\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.74880232464 0.39064275768\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.73408059917 0.395804769323\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.71956969282 0.400936745545\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.70994938211 0.403679497942\n",
      "most recent validation loss: 3.59629165152 0.141205496115\n",
      "training loss: 1.69960624881 0.406807043178\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.69054929507 0.41083412288\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.68201522228 0.411550005303\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.6763903681 0.41600963054\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.66979866909 0.418351106623\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.66170639698 0.420734592182\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.65381865138 0.426222269619\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.64671174669 0.424112971869\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.64153904081 0.425841386302\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.63486630565 0.43023748445\n",
      "most recent validation loss: 1.80669598035 0.368751625261\n",
      "training loss: 1.62709529168 0.434286068609\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.6218928755 0.436307949\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.61599583104 0.436968504583\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.61450486801 0.436446616476\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.60575807766 0.441046319074\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.60361879901 0.440847794526\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.59729649346 0.443336819757\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.59262744822 0.446102179364\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.58684344164 0.448762107534\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.57918250011 0.449277698166\n",
      "most recent validation loss: 1.75384482831 0.391139988404\n",
      "training loss: 1.57582485818 0.449521333106\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.5718184371 0.454972877527\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.56778257569 0.450084524665\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.56135817857 0.457180603423\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.55961436956 0.455931341463\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.55558881548 0.45625638324\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.54974262645 0.460375048714\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.54449859595 0.462077740221\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.5415093395 0.463197195993\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.53857499399 0.466825661805\n",
      "most recent validation loss: 1.76622743822 0.385105613712\n",
      "training loss: 1.5355679817 0.467841484359\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.53009010184 0.472259426424\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.52537975307 0.474301784258\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.51983186091 0.475600706181\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.51593523334 0.474823221205\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.51263351585 0.474668183356\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.51009048232 0.476142076329\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.50461895477 0.475264073213\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.50107124602 0.480275912766\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.49824804043 0.482377457191\n",
      "most recent validation loss: 1.72762488169 0.400233545943\n",
      "training loss: 1.49363093435 0.484893798502\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.48950280508 0.486749663375\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.48921497362 0.489365213088\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.48253023384 0.489075378465\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.47970869266 0.492735138317\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.47509512685 0.492156696001\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.47203379792 0.494068308775\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.46817387888 0.496113232269\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.47017981121 0.495700878654\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.46025462884 0.498826967098\n",
      "most recent validation loss: 1.75881639794 0.393784648341\n",
      "training loss: 1.45964074661 0.502628009821\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.4561402706 0.502106976852\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.44835600671 0.507004959498\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.44941925539 0.508660960504\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.44442243668 0.505815414544\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.44190378642 0.509679263326\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.43902933121 0.508408845512\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.43610567663 0.510441064693\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.43466477907 0.50904470753\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.42927400422 0.515722262552\n",
      "most recent validation loss: 1.74102378108 0.410136406042\n",
      "training loss: 1.42642508337 0.519601241428\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.42304454346 0.518672205389\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.41951453737 0.519427930166\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.41818059752 0.51851550023\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.41598817574 0.518470587877\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.40951304799 0.524470965617\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.40834422897 0.524676792498\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.4066538498 0.527332386435\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.40312540348 0.526252924033\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.39750926516 0.531219406793\n",
      "most recent validation loss: 1.82684063534 0.397624069558\n",
      "training loss: 1.39788768864 0.526148508888\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.39376438651 0.527554082295\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.39237770729 0.530422620269\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.39192765748 0.530199303574\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.3857081634 0.531066033407\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.38090570508 0.534181182138\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.38050077963 0.534531122676\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.37854178065 0.535510470522\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.3768459695 0.537335438278\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.37256214226 0.536603959135\n",
      "most recent validation loss: 1.76965074503 0.407894721957\n",
      "training loss: 1.36923622152 0.541424838981\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.36704723266 0.54041071736\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.36420873145 0.545440561916\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.36199330217 0.543652599677\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.36047395061 0.541631354775\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.35537397058 0.547148157698\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.35558207266 0.546107025149\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.35134225627 0.546449471777\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.34890123107 0.550228449415\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.34611287158 0.549413659813\n",
      "most recent validation loss: 1.80480044122 0.411277010247\n",
      "training loss: 1.98015646573 0.359089300394\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.76366325183 0.425016205903\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.71064708259 0.440189021601\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.68602767667 0.446061554438\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.66535690273 0.458957338851\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.65117700969 0.459586543976\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.63738449053 0.459245882814\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.6255624159 0.465392494965\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.61490068839 0.465955051883\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.60766073829 0.472243723914\n",
      "most recent validation loss: 2.94647250404 0.146367212137\n",
      "training loss: 1.59488118321 0.474325563767\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.5902456398 0.475300379228\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.57949516496 0.481565809707\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.5757146696 0.477330039841\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.56505710066 0.480293731947\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.55882042715 0.485833029445\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.55077822945 0.484372251953\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.54489310725 0.485417085887\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.53815220265 0.489805074686\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.53152191104 0.492104352981\n",
      "most recent validation loss: 1.83609862652 0.396082246336\n",
      "training loss: 1.52951913256 0.492372466274\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.51939062284 0.494647891561\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.51579431754 0.495364305737\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.50957811336 0.493277744402\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.50625681907 0.499956237747\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.50208893035 0.498803509396\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.49865687377 0.502475558504\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.4953379251 0.502998946471\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.48874007051 0.50494731675\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.48478929952 0.50434280753\n",
      "most recent validation loss: 1.87235698886 0.384791858259\n",
      "training loss: 1.48089842397 0.506799114278\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.47740634564 0.507067691472\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.47405107522 0.508351791123\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.46404679652 0.514701743792\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.46283910135 0.51274467062\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.46114447812 0.512409436606\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.45702746207 0.51437507187\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.45187408681 0.516861007745\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.44816284753 0.519480165057\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.44755179609 0.517479717235\n",
      "most recent validation loss: 1.7842155459 0.407474756278\n",
      "training loss: 1.44258667161 0.517855965253\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.43961613361 0.517951650847\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.43679124951 0.522804345754\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.4298799848 0.522916598347\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.43066603442 0.523835435875\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.42554014794 0.52701526331\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.42081427279 0.525373930597\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.41726822473 0.528904373924\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.41782285995 0.526108301596\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.41302789099 0.528778193017\n",
      "most recent validation loss: 1.87453932423 0.404648367319\n",
      "training loss: 1.4078174168 0.532844245621\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.40520867537 0.536813329687\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.40305230525 0.536954703545\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.39947074328 0.539362458137\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.39670091065 0.538129770999\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.39012389341 0.540553766428\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.38747279534 0.540976915483\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.39183569541 0.539011001212\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.38461293902 0.542798526561\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.38264860849 0.54355190306\n",
      "most recent validation loss: 1.82215764446 0.40723687965\n",
      "training loss: 1.37625278678 0.545539412443\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.37777382757 0.54395350775\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.37101500485 0.547209409283\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.36948832379 0.547681996531\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.36407452516 0.54885035553\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.36255217972 0.550257259467\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.36239529509 0.549810592807\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.35910774918 0.554050910329\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.35668898408 0.551694534821\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.35724640479 0.552804083706\n",
      "most recent validation loss: 1.89413764812 0.403586199712\n",
      "training loss: 1.35033553845 0.552995532973\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.35198091513 0.555577801422\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.34387580019 0.559537092862\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.34339037162 0.559054040114\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.33999543681 0.559006533108\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.33795136579 0.558202238649\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.33580079536 0.559500818317\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.33007947765 0.562995241045\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.32957014471 0.564379876752\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.32666057859 0.561518385449\n",
      "most recent validation loss: 1.88400038999 0.402194312226\n",
      "training loss: 1.32473506199 0.565573111943\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "training loss: 1.32294212231 0.56567132186\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "training loss: 1.31781725985 0.565700932378\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "training loss: 1.31515994247 0.565608923861\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "training loss: 1.31038992928 0.57160881873\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "training loss: 1.30905697363 0.571118716545\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "training loss: 1.30705655523 0.571671388079\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "training loss: 1.30344533699 0.571327730745\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "training loss: 1.30115611372 0.57344755191\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "training loss: 1.29824029008 0.576828667156\n",
      "most recent validation loss: 1.90406455895 0.413932931644\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.22      0.25       958\n",
      "          1       0.16      0.55      0.25       111\n",
      "          2       0.26      0.26      0.26      1024\n",
      "          3       0.55      0.64      0.59      1774\n",
      "          4       0.33      0.29      0.30      1247\n",
      "          5       0.55      0.59      0.57       831\n",
      "          6       0.41      0.33      0.37      1233\n",
      "\n",
      "avg / total       0.40      0.41      0.40      7178\n",
      "\n",
      "accuracy:  0.40749512399\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 200, 100, 50, (7, 'softmax')], weighted=True)\n",
    "for tr, val in net.itertrain((train[0], train[1], weights), (test[0], test[1], weights_t),  algo='layerwise', \n",
    "                             weight_l1=10):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['loss'], val['acc'])\n",
    "    \n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3 hidden layers with 1000, 250 and 50 hidden nodes**\n",
    "* ** Hinge loss**\n",
    "* **'softmax' for the output layer, 'relu' for others**\n",
    "* **Nesterov's accelerated gradient with learning rate 1e-3, momentum 0.9**\n",
    "* **L1 regularization with parameter value 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.525811373658 0.158087878979\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.395307459013 0.173671218669\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.331003896033 0.176750917313\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.295931617651 0.174307866981\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.274073515502 0.166884119172\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.258601724811 0.16843741613\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.245868389777 0.174756387183\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.234754372552 0.172722729067\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.224588139541 0.171909215073\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.21494552481 0.175498216185\n",
      "most recent validation loss: 1.14257378383 0.151445437405\n",
      "training loss: 0.205738157445 0.174789409699\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.197099621627 0.168131896124\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.188855591209 0.175887335592\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.181107800155 0.174206793609\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.173781206442 0.176053440669\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.166899822592 0.177233353605\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.160343501908 0.173303309848\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.15413467947 0.173010186917\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.148284322939 0.174789159061\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.142869980358 0.175800385006\n",
      "most recent validation loss: 0.209422182912 0.162603550049\n",
      "training loss: 0.137739242947 0.1772005586\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.132938990574 0.174107978406\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.128493893888 0.171729459016\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.124276632844 0.178562918911\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.12029835486 0.172857239246\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.116578895945 0.175886753213\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.113140365661 0.171684274191\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.109815493052 0.173294714663\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.106732342929 0.177492331746\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.103845741153 0.173396347165\n",
      "most recent validation loss: 0.139901889698 0.166191590194\n",
      "training loss: 0.101000906546 0.173175872489\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.0983536227498 0.172079270872\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.0958188353698 0.180673753181\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.0934827584961 0.173880710565\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.091191493434 0.174149642137\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.089046085034 0.174905870848\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.087043222912 0.174154631038\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.0851247585576 0.175681359203\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.0832679094964 0.170895672039\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.0815119337523 0.179042863013\n",
      "most recent validation loss: 0.102518262985 0.192088303757\n",
      "training loss: 0.079831431269 0.177334326236\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.0783462855057 0.173265578804\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.0767601052157 0.177853719062\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.075337031295 0.176837528432\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.0738226147629 0.17674145241\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.0725045681379 0.174097341366\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.0711658231019 0.175827951625\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.0698253319467 0.180407723124\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.0687057412717 0.173257687313\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.0674531365669 0.176702641832\n",
      "most recent validation loss: 0.0809410776006 0.159684446055\n",
      "training loss: 0.0663433153078 0.174195856816\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0651880360935 0.174191318808\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0642189165879 0.174533134912\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0631820990182 0.176940076928\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0621807617421 0.171918275645\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0611271495818 0.17939792611\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0602500161713 0.176972147326\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0593199645446 0.177492384806\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0585298492747 0.17533540373\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0576726853387 0.178760488551\n",
      "most recent validation loss: 0.0667768907043 0.190923754277\n",
      "training loss: 0.0568633158097 0.175048368293\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0559367229107 0.183834508482\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0552350402114 0.178240094078\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0544885293076 0.173485810532\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0537555429296 0.176228636123\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0529411699925 0.180314704433\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0522730009252 0.178722790849\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0517043290294 0.172638581086\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0509410402919 0.179148738616\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0502630184109 0.179497702195\n",
      "most recent validation loss: 0.0571242111086 0.176973327607\n",
      "training loss: 0.0497206288012 0.176251449501\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0491221105774 0.173730504092\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0484507947529 0.171348574426\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0478672541788 0.178961178949\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0473749871021 0.176420234255\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0468194180235 0.178180217829\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0462691109967 0.177188630148\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0457384445234 0.176835849492\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0452344145432 0.180359327842\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0447727890835 0.178517937806\n",
      "most recent validation loss: 0.0495805522236 0.198483083245\n",
      "training loss: 0.0443078025948 0.17602436792\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.0438531328428 0.179138386079\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.0433827181955 0.177050437329\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.0429257218814 0.173751744334\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.042449761041 0.18674311912\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.0420712490038 0.181353011909\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.0417316428497 0.177617467619\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.0412418237421 0.184609861009\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.0409167822086 0.18076817751\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.0405883310072 0.178425369065\n",
      "most recent validation loss: 0.0439952804585 0.207838887765\n",
      "training loss: 0.040220803817 0.180315375054\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.0398619918013 0.179028312855\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.0395212490906 0.177570233321\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.039198809318 0.179727699146\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.0388812564677 0.174390687198\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.0385413351921 0.178688848415\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.0382384264841 0.178702414265\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.0379800524969 0.178156191122\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.0376504246609 0.187307188067\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.0374234721468 0.173081166155\n",
      "most recent validation loss: 0.0399966895581 0.204241338497\n",
      "training loss: 0.0371740638707 0.177696383531\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.036834000915 0.180816045496\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.036603427159 0.178784202712\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.0363386267183 0.177922397431\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.0361301304419 0.177756705257\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.0358481070908 0.177065414314\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.0355814514524 0.180742228505\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.0353643510579 0.183188845591\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.0351299211142 0.178515533402\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.0349071988911 0.173847217345\n",
      "most recent validation loss: 0.0379333023167 0.173646234801\n",
      "training loss: 0.0346944958133 0.179555721227\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.034494792051 0.173199490208\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.03426988649 0.176017594234\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.0340831724373 0.176908154207\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.0338333372083 0.184765414736\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.0336553159025 0.176111290884\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.0334864033757 0.170521111471\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.033263924104 0.173142808469\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.0330294510329 0.177297059583\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.0328011154483 0.185902992937\n",
      "most recent validation loss: 0.0344199854294 0.204038828122\n",
      "training loss: 0.0326726482048 0.175050725922\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.0324727436543 0.179126971683\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.0322622317074 0.179175571738\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.0321231454791 0.173872470035\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.0319337277593 0.179336409276\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.0317003538854 0.185242429073\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.031597202837 0.176631445895\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.0314498902122 0.175648253084\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.0312755718987 0.176189457416\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.0311152832554 0.178707189296\n",
      "most recent validation loss: 0.0337584009661 0.173835428542\n",
      "training loss: 0.0308947618036 0.175068670263\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0307843988569 0.179220991691\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0306083721754 0.176389469673\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0304324963763 0.181178363465\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0303258733629 0.17099346963\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0301698719022 0.175766506099\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0300269082808 0.176366854064\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0298929301676 0.17611396424\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0297813721008 0.172828725845\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0296373423559 0.174724764599\n",
      "most recent validation loss: 0.0312476590671 0.216601532218\n",
      "training loss: 0.0295037117222 0.175918734794\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0293488340023 0.175390219533\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0292203872548 0.175550773743\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0291245512595 0.177956084541\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0289519105187 0.179874653308\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0288088410622 0.180926653916\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0286996803257 0.177574336984\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0285575753812 0.176650917003\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0284790902262 0.170356922593\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0283231686628 0.176752186893\n",
      "most recent validation loss: 0.0297612505175 0.142647355288\n",
      "training loss: 0.0281918167458 0.17526457003\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.0281035368472 0.174177124925\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.0279638278483 0.169592319467\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.0278149751428 0.175440634141\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.0276705093477 0.176308412289\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.0276071576879 0.178037158521\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.0274449826145 0.170479945111\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.0273562055746 0.173374071957\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.027202841041 0.175817370556\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.0270844743371 0.174471037884\n",
      "most recent validation loss: 0.0279878731888 0.147549553283\n",
      "training loss: 0.0269799539645 0.174529064256\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0268471387121 0.174344055496\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0267453934455 0.170233787429\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0266080041316 0.178379482942\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0265253295558 0.169180076937\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0263915924066 0.172392698427\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0263154141535 0.168460752868\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0261586058713 0.170835398451\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0260320242329 0.177928460525\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0259379680333 0.164783232696\n",
      "most recent validation loss: 0.0267089875161 0.169110003882\n",
      "training loss: 0.0258172064633 0.168553960177\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0257396479681 0.168686276138\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0255620748376 0.17385766969\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0254391706234 0.167255078145\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0253584587825 0.175183866239\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0251943926266 0.174299492682\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0251041964043 0.16620924032\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0250298248701 0.166973071518\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0248876197919 0.168734609653\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0247966097974 0.167527153896\n",
      "most recent validation loss: 0.0252380142681 0.181565910855\n",
      "training loss: 0.0246418778543 0.16957129117\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.0245693259589 0.164288781797\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.0244413634423 0.165747531515\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.0243090557509 0.163061919508\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.0241539007613 0.164930667872\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.0240654193731 0.162407048533\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.02394657586 0.166622353929\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.0238166304715 0.166834168949\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.0237360969047 0.16429570285\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.0235842749535 0.166564611274\n",
      "most recent validation loss: 0.0245650389426 0.151745491076\n",
      "training loss: 0.0234581464808 0.168806777917\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.0233941987165 0.161163467081\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.023289913782 0.161617530803\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.0231093346888 0.159795083903\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.0230365933445 0.159189075636\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.0228944271616 0.157437345726\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.0227765792723 0.163986205228\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.0226939701867 0.157886542848\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.0225410860085 0.159529191087\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.0224358110188 0.158644828643\n",
      "most recent validation loss: 0.0233722426731 0.173966768393\n",
      "training loss: 0.0223485960878 0.159729781414\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.0222259147416 0.159591261073\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.0220973771218 0.15904572454\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.0219871073275 0.158127126684\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.0218727542069 0.159713346081\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.021801304281 0.15362764938\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.0216590982012 0.158947011208\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.0215734885151 0.152344088923\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.0214581422847 0.151318649783\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.021323828389 0.152334092954\n",
      "most recent validation loss: 0.0219486563589 0.161119233392\n",
      "training loss: 0.0212213709994 0.154766502321\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0211086749349 0.148407547232\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0209993537232 0.149443238833\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0208782079256 0.150055921989\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0208078240916 0.148963996174\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0206661803118 0.146551820906\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0205554338508 0.147262649272\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0204215135041 0.148303248685\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0203196931234 0.146156644139\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0202130143491 0.146524736898\n",
      "most recent validation loss: 0.0214489686201 0.147304998242\n",
      "training loss: 0.0201122545745 0.1412475797\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.0200163190109 0.143121671847\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.0198935749546 0.144083655825\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.0198123331061 0.146393876292\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.0197261393143 0.145716123212\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.0196348933046 0.144579495653\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.019554699561 0.142749477552\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.019486147013 0.136237172638\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.0193874478062 0.140789899454\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.0192993263159 0.146013230987\n",
      "most recent validation loss: 0.020431588087 0.151445437405\n",
      "training loss: 0.019246515385 0.141176649812\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.0191144992363 0.144180819414\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.0190281805985 0.143588671971\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.0189540669106 0.144661185446\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.0188713191274 0.142422792418\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.0187905187432 0.141867337684\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.0187156510635 0.141839558262\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.0186568951546 0.141824565563\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.0185501875396 0.143647036397\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.0184698654331 0.144819937239\n",
      "most recent validation loss: 0.0186791595796 0.151445437405\n",
      "training loss: 0.018395749577 0.142681620299\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.0183444854967 0.139554647908\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.0182556042972 0.144272915368\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.0181986219719 0.144461673182\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.0181179774745 0.141266342403\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.018035576852 0.142571227097\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.0179452253541 0.145376260752\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.0178824303091 0.140451269957\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.0178053500644 0.145099580484\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.0177357805305 0.143891874936\n",
      "most recent validation loss: 0.0181410286941 0.147304998242\n",
      "training loss: 0.0176438274955 0.138100563888\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0175843447285 0.143842833819\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0175361325666 0.144464790121\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0174340323034 0.14401445427\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0173786511408 0.141328321713\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0173092359434 0.14138985933\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0172407389684 0.141751606112\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0171452853453 0.144259375263\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0171075252924 0.138807985428\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0170054560593 0.142508154468\n",
      "most recent validation loss: 0.0179754084401 0.141205496115\n",
      "training loss: 0.0169134861395 0.145184108404\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0168796156782 0.140570399639\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0168065743813 0.141784334769\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0167362130513 0.143985181853\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0166707481099 0.141010628668\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0165839002648 0.139926973583\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0165182274256 0.146713389323\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0164691077807 0.140906671528\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0163929839323 0.139247526889\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0163354574833 0.141847613929\n",
      "most recent validation loss: 0.0169645571698 0.150377797355\n",
      "training loss: 0.0162532565582 0.147631373926\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0161823063444 0.141907926946\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0161350330312 0.139219331598\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0160458534125 0.143910107116\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0159904273176 0.144694569869\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0159529556649 0.140485774556\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0158698823611 0.14427433846\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0158028519145 0.140754843863\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0157369414897 0.142227873\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0156760322186 0.145858560531\n",
      "most recent validation loss: 0.0158398268178 0.146367212137\n",
      "training loss: 0.0156116695301 0.1462709781\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.015527464163 0.140943058607\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.0154704155229 0.139166830425\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.0154197003781 0.141361923056\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.0153531763866 0.138610294141\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.0152820735564 0.145402791077\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.0152192591367 0.144385990662\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.0151484664004 0.141981132611\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.0151053777268 0.143791607628\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.0150275727097 0.140628922345\n",
      "most recent validation loss: 0.0158466129712 0.147304998242\n",
      "training loss: 0.0149515079844 0.146372756401\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0148827140737 0.144148456389\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0148550487365 0.13852403008\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0147627436237 0.140607538729\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0146878850505 0.145028792083\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0146482374067 0.14185347752\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0145833876293 0.143737496248\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0145123324824 0.145395798048\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0144565089305 0.137350620107\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0143829278543 0.143131911695\n",
      "most recent validation loss: 0.0153134917292 0.141205496115\n",
      "training loss: 0.0143437510103 0.146518636486\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0142797288798 0.144345881011\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0142178012011 0.144316062208\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0141476327433 0.141361093234\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0140730660929 0.148494822906\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0140389405905 0.141769487701\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0139675005923 0.14545183647\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0139206947134 0.141810997612\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0138410261677 0.143523918522\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0137900723474 0.147715353155\n",
      "most recent validation loss: 0.0141898214776 0.117311205622\n",
      "training loss: 0.0137470837406 0.142987000011\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.0136648459528 0.143997786487\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.0136205154427 0.147161812089\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.0135681332358 0.140594006579\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.0134907577103 0.146403635248\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.013445855663 0.147582904953\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.0134007679976 0.144681101463\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.0133228942337 0.14372562645\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.0132809612548 0.14293313451\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.0132346083146 0.143488032097\n",
      "most recent validation loss: 0.0135887994375 0.147304998242\n",
      "training loss: 0.0131897886629 0.14198527188\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.01313092412 0.143162132246\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.013060764607 0.146927613923\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.0130055760016 0.14344705038\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.0129576648902 0.146156083722\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.0129102750784 0.146799386298\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.0128673331673 0.14215827783\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.0128116052058 0.147316939213\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.0127335575603 0.146093522391\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.0126999644132 0.14342136709\n",
      "most recent validation loss: 0.013212028648 0.146367212137\n",
      "training loss: 0.0126601190095 0.140758692752\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.0126162169464 0.144895386992\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.0125764068551 0.1445255996\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.0124985485351 0.141923151726\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.0124499690899 0.1433156499\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.0124073582929 0.143779205671\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.012350323131 0.145248553096\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.0122885980887 0.14375551752\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.0122600916946 0.144148213482\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.0121816526274 0.143642357415\n",
      "most recent validation loss: 0.0126777192764 0.146367212137\n",
      "training loss: 0.0121488506448 0.145257935709\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.0121035594698 0.144736804536\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.0120747315866 0.145726016167\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.0120065607685 0.147296346044\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.011974700536 0.14359096463\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.0119060163422 0.144373080436\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.0118368717837 0.146438915537\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.0118193165155 0.143217541777\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.0117540303059 0.141642227109\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.0117026288997 0.143724941337\n",
      "most recent validation loss: 0.0120875410755 0.151445437405\n",
      "training loss: 0.0116517605578 0.145495160307\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.0116156025848 0.145220655519\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.0115539757331 0.140477354255\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.0115124197206 0.146319035281\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.0114596259736 0.144657388713\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.0114175885932 0.146258095106\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.011376700605 0.141823976413\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.011329877524 0.147300807844\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.0112856013813 0.142642742961\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.0112359315509 0.14594258987\n",
      "most recent validation loss: 0.0113301504682 0.151445437405\n",
      "training loss: 0.0111809190071 0.144825925805\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.0111234690777 0.142944323983\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.0110845056876 0.1412279635\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.0110585879429 0.141221262683\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.0110090468515 0.139768038483\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.010956546141 0.14078587161\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.0109030983559 0.145670929922\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.0108752537598 0.149372567067\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.0108253990994 0.143392778203\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.0107766613494 0.14421468731\n",
      "most recent validation loss: 0.0109372963343 0.150377797355\n",
      "training loss: 0.0107240363673 0.145781184011\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.0106721104007 0.146919812027\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.0106394643038 0.143018734975\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.0105960896729 0.142537423313\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.0105495767108 0.141100986898\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.0105107847681 0.14083680222\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.0104435569186 0.145073360187\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.0104066598826 0.147178713592\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.010387443854 0.142812334641\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.0103458071823 0.144809336715\n",
      "most recent validation loss: 0.0108452094352 0.146367212137\n",
      "training loss: 0.0102953299203 0.141727818223\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.0102550425158 0.138850977984\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.010192714783 0.145096529172\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.0101597804522 0.138954266971\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.0101260833233 0.141277797876\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.0100709343319 0.14318771979\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.0100310506508 0.142565015922\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.00999077207372 0.143421088293\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.00995902007611 0.140920924113\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.00992905049178 0.14227335751\n",
      "most recent validation loss: 0.0101727357367 0.141205496115\n",
      "training loss: 0.0098739340226 0.145813493408\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.00983677924939 0.14218823433\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.0097767130873 0.136821006437\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.0097245547 0.145210797098\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.00969064181144 0.147615156169\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.00966513449561 0.145842486278\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.00961835531243 0.144354824969\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.00957042352074 0.141165371802\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.00953139550949 0.148963887993\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.00950668944944 0.141607623872\n",
      "most recent validation loss: 0.00986323653235 0.151445437405\n",
      "training loss: 0.00946984341626 0.144046863579\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.009417770908 0.146937173414\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.00938306621105 0.146200347173\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.00933326425488 0.143346958118\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.00929961556138 0.146473791669\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.00925567267755 0.145532061636\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.0092332522881 0.141644053087\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.00919253447007 0.139673792537\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.0091304167459 0.143914054048\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.00909250541744 0.147931703894\n",
      "most recent validation loss: 0.00920281567894 0.150377797355\n",
      "training loss: 0.00906232645567 0.144880637028\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.00901117190453 0.142701148918\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.00899529425205 0.141015960347\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.0089429145197 0.143051653464\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.00890124203881 0.143445521943\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.00886145373783 0.144684334216\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.00881539464343 0.14531441416\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.00877849296979 0.145237337339\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.00875460585886 0.14408611003\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.00872848222245 0.14324007983\n",
      "most recent validation loss: 0.00902923148646 0.151445437405\n",
      "training loss: 0.00867430695254 0.147522821884\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.0086420212478 0.147968497307\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.00858725742946 0.149240920009\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.0085853485308 0.140276787598\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.00851781245298 0.144381672948\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.00849872627281 0.144609969328\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.00843634649839 0.142415207092\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.00842385420877 0.146361478108\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.00836998510094 0.14544603022\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.00833938374181 0.144805257802\n",
      "most recent validation loss: 0.00865996054343 0.151445437405\n",
      "training loss: 0.00830639954587 0.146331572973\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00826338203853 0.145384385998\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00826015296217 0.135503982596\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00820816043038 0.142752946951\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00817339995476 0.141416803133\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00812382820451 0.143123337388\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00808226459728 0.139260778476\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00807418880958 0.141702390813\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00802865737555 0.142284091406\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00799438062811 0.146358022024\n",
      "most recent validation loss: 0.00811152106779 0.145987853124\n",
      "training loss: 0.00796253430077 0.143277707139\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00792636204492 0.141913404289\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00788762379564 0.14266941504\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00784745646896 0.139224238411\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00779971215155 0.141019445251\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00778090595878 0.146658406203\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00774230251134 0.144780577817\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00770218380184 0.145553579584\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00768180722649 0.144033765464\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00766356704634 0.141183326382\n",
      "most recent validation loss: 0.00817948220351 0.147304998242\n",
      "training loss: 0.00762240156289 0.143208641945\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.00757027124013 0.144761482259\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.00754196019424 0.144551042559\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.00751210684131 0.145497495906\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.00749009781673 0.139128893124\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.00745869426182 0.137064553666\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.00741049622538 0.147519423217\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.0073861789441 0.145380357652\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.00735742525325 0.142847168797\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.00730802555341 0.141261338318\n",
      "most recent validation loss: 0.00753334783323 0.145987853124\n",
      "training loss: 0.00728385992105 0.145165522137\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00724961233632 0.142636240096\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00722380769091 0.144155639491\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00719331213816 0.141568178052\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00716071503963 0.145835469395\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00713476918283 0.147181715045\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00710428783868 0.141523097261\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00705998551491 0.143762347644\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00704248514383 0.145554608972\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00699986768388 0.142611690882\n",
      "most recent validation loss: 0.00715633607619 0.141205496115\n",
      "training loss: 0.00695707694371 0.147574176096\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.00693960258851 0.141748501146\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.00690817378201 0.143571457181\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.00687997764691 0.139409339887\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.0068427108628 0.142394166079\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.00682190356126 0.140596918779\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.0067847085755 0.140316554486\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.00675405772331 0.149664101127\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.00671457427876 0.144882469061\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.00667905672251 0.144629921759\n",
      "most recent validation loss: 0.0068641311326 0.145987853124\n",
      "training loss: 0.00666878853671 0.141156898028\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.00663790026172 0.144228819911\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.00660078804366 0.14389911458\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.00658272307399 0.143103375604\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.00653059589695 0.149832786434\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.00651498257906 0.140339718063\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.006492133074 0.141334579876\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.00645592970702 0.142897835341\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.00642334915381 0.14758493856\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.00640632402895 0.141063955029\n",
      "most recent validation loss: 0.00654540538179 0.145987853124\n",
      "training loss: 0.00637446128135 0.14661637723\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00634297118947 0.140275765471\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00630837163132 0.146146690286\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00628817653553 0.144885484176\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00624471194813 0.142539303571\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00623485651948 0.143249050022\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00618899942723 0.145896718866\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00618889937449 0.141795098485\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00613771615424 0.145716913933\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00612797631575 0.143399121251\n",
      "most recent validation loss: 0.00643766759793 0.117311205622\n",
      "training loss: 0.00610798979507 0.137987226738\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.00605034085465 0.150341440881\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.00605559479853 0.143230552445\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.00601761121975 0.143018566465\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.00598036697808 0.147408201126\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.00596819983781 0.138228199681\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.00593476234058 0.139432511772\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.00590612014455 0.14413058288\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.00586914518253 0.145255212424\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.0058659336341 0.147752856193\n",
      "most recent validation loss: 0.00643284972933 0.151445437405\n",
      "training loss: 0.00582435620343 0.145708959064\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.0057966921452 0.142528152914\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.00578325215686 0.138726924781\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.00575052649955 0.141738868688\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.00573783229406 0.14205991644\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.00568784115058 0.146327789715\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.00569427382959 0.141441082694\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.00565782815755 0.139661483081\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.0056153605951 0.148081052007\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.00559597897095 0.146901959908\n",
      "most recent validation loss: 0.00566971320212 0.117311205622\n",
      "training loss: 0.00558056412201 0.146465178908\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.00555093798567 0.141756467587\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.00552909112413 0.142989303105\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.00549028690477 0.143499850661\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.00546540056257 0.145097975946\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.0054496130544 0.142111334844\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.00541818418321 0.145530701612\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.00539076737974 0.147848688435\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.00536682111445 0.145155933243\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.00537565407728 0.146553777288\n",
      "most recent validation loss: 0.00566528756691 0.146367212137\n",
      "training loss: 0.00533009608804 0.143709138594\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.00529004898452 0.145077944007\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.00526284356502 0.149009248892\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.00525450440285 0.14165435756\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.00523877439227 0.144147709784\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.0051876078675 0.143632570268\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.00517800928404 0.146853595006\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.00517104006107 0.142531462315\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.00511586099568 0.143542292524\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.00511924689294 0.143495086442\n",
      "most recent validation loss: 0.00554800598416 0.151445437405\n",
      "training loss: 0.00508533846642 0.144458878373\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00506496279658 0.145544958241\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00503981630719 0.143609140971\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00501203170805 0.146795232989\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00500213027196 0.150434200959\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00497437724286 0.143194267213\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00495620948996 0.146142598626\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00494589792451 0.140254258643\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00492402624638 0.141846742951\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00489288188229 0.145531233205\n",
      "most recent validation loss: 0.00511564124308 0.151445437405\n",
      "training loss: 0.00485361401315 0.151327398307\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.0048389497912 0.145558661339\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.00481197315698 0.143163890024\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.00481104771633 0.143355221799\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.00476429385641 0.144840466321\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.0047525674764 0.145764042685\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.00474308868428 0.141853911442\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.00471418527074 0.144331913277\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.00470950288343 0.145567465206\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.00465323569319 0.145081215276\n",
      "most recent validation loss: 0.00544986358061 0.147304998242\n",
      "training loss: 0.00464610310573 0.14289667013\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.00461571119823 0.147108299243\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.0045941200967 0.145578916209\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.00457173459963 0.140163457922\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.00454333010564 0.150994163774\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.00453688557915 0.146253260211\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.00452558309553 0.147619400907\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.00449888436219 0.146520851052\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.00445990645069 0.143691650631\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.00446207213435 0.142271276934\n",
      "most recent validation loss: 0.00444473279062 0.141205496115\n",
      "training loss: 0.00443360939992 0.141591045932\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.00439137175776 0.143677278356\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.00439532582382 0.141166443446\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.00439054795489 0.142861562205\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.00434523677355 0.14591011802\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.00434084194799 0.141083757931\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.0043259377669 0.142606778166\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.00429699166279 0.144124679739\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.00428340823231 0.137042669966\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.00425238754636 0.149376447961\n",
      "most recent validation loss: 0.00482931807605 0.117311205622\n",
      "training loss: 0.00424911859927 0.146396376706\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.00422125608742 0.147490837076\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.0042146174374 0.146661912408\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.00418457549915 0.145552300892\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.00416114394282 0.140173913104\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.00415331555667 0.145959030287\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.00413844582398 0.14249305879\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.00410759503273 0.137636454224\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.00407410186475 0.14626455624\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.00405504946092 0.141718442134\n",
      "most recent validation loss: 0.00445531532681 0.147304998242\n",
      "training loss: 0.00404131613707 0.14392123116\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00403186049124 0.13970148727\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00398320986704 0.145957642992\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00398684252221 0.146324501185\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00399577243442 0.142961640458\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00398214168635 0.136184356854\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00392933469414 0.143231265763\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00393762842043 0.142001451147\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00390949841794 0.140632839913\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00388559176662 0.142532134167\n",
      "most recent validation loss: 0.00419591107369 0.147304998242\n",
      "training loss: 0.00388387631887 0.145121173422\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00385704976107 0.138243935815\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00381714960273 0.147759378785\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00382675007535 0.140741725029\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00381227971126 0.139579758649\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00379698750336 0.143643975441\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00376345422582 0.142108870328\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00375114875295 0.141322422243\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00372425315869 0.141000977704\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00370849196775 0.140592282799\n",
      "most recent validation loss: 0.00365143857526 0.146367212137\n",
      "training loss: 0.00368463117923 0.145016045648\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.00368237474593 0.144045931717\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.00367668001952 0.138728403042\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.00363413609631 0.144353597663\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.00361534485995 0.149907214542\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.0036122840442 0.147317638927\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.0036097388944 0.145688203909\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.00357260201405 0.141712580737\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.00356448492817 0.147330374204\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.00353014505062 0.146325594843\n",
      "most recent validation loss: 0.00357840835251 0.141205496115\n",
      "training loss: 0.00352680175125 0.144560735909\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.00350353140297 0.144311017152\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.0034980619103 0.141330125767\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.00348459247131 0.143018545403\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.0034733205905 0.145633801367\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.00345436071345 0.143864277083\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.00344155224642 0.143581459658\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.00342227237648 0.14235657052\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.0033992971578 0.139808449003\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.00339674860225 0.139507431916\n",
      "most recent validation loss: 0.00396780862983 0.141205496115\n",
      "training loss: 0.00337428469716 0.140918490006\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00334156011823 0.145911561669\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00333536759286 0.146550655259\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00332336887894 0.141651701149\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00331009544299 0.140014482274\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00329418240525 0.146000595834\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00328402323903 0.143959246347\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00327507754602 0.143570827993\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00325218656957 0.143296318254\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00324643389436 0.144042279558\n",
      "most recent validation loss: 0.00325012487228 0.147304998242\n",
      "training loss: 0.00322498435522 0.141379489951\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.00319716740972 0.144764618096\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.00318500176895 0.146270659229\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.00316078582996 0.147421970121\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.00318757551064 0.141331405708\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.00315577509338 0.14534608525\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.00311933009293 0.145335570349\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.00312717026073 0.141976014832\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.00311006981254 0.145374820764\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.0031041835789 0.143447699127\n",
      "most recent validation loss: 0.00341999946492 0.151445437405\n",
      "training loss: 0.00307268563406 0.145052530653\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00306200591255 0.144484530024\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00304926883176 0.136483217505\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00304050547043 0.142139982321\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00303244977519 0.1475113837\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00302221885063 0.144461067044\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00301158827198 0.140332458575\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00300004231248 0.141887906067\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00297497086576 0.144007566076\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00296395176984 0.144389760428\n",
      "most recent validation loss: 0.00306611521453 0.150377797355\n",
      "training loss: 0.00293728624044 0.140852821177\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00294337972642 0.13920223956\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00293266825252 0.146665442797\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00289169745213 0.140960300705\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00289715759105 0.138306437485\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00288138771121 0.142512689541\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00287768525068 0.143369464584\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00284569310315 0.145174994472\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00284717268626 0.143257245654\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00282795287252 0.138778303143\n",
      "most recent validation loss: 0.00295575592095 0.145987853124\n",
      "training loss: 0.00282234512363 0.141167524328\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.00279211706107 0.144397633902\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.00278829721361 0.145707116996\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.00278922327664 0.140957325648\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.00276779243541 0.14073035173\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.00277376696205 0.148905710596\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.00273887414168 0.141926318261\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.00274158342435 0.144381263452\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.00269473027452 0.143602280908\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.002706184848 0.140765445609\n",
      "most recent validation loss: 0.00279974953886 0.117311205622\n",
      "training loss: 0.00270041967606 0.14157461008\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.00267289692792 0.144501283029\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.0026664966076 0.142775923232\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.00265379233176 0.144579007469\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.00265661197575 0.141388089546\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.00262341773671 0.138249107962\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.00264011498911 0.144449745053\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.00261314286212 0.14178281482\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.00260504527908 0.146332645016\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.00255472362192 0.145556533312\n",
      "most recent validation loss: 0.00251689165732 0.145987853124\n",
      "training loss: 0.00256855917674 0.144466494092\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00255777147985 0.145981932787\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00254271176579 0.144804755457\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00255161628997 0.144868868114\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00251511415429 0.14243919169\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00251861592118 0.141733519519\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00250192857577 0.143626314414\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00249489545392 0.140588026112\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00245937490667 0.148446196888\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00247058779734 0.142338921586\n",
      "most recent validation loss: 0.0027455100753 0.147304998242\n",
      "training loss: 0.00246633614798 0.143216383509\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00244192113789 0.145735766568\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00244744616069 0.144240341204\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00241626252551 0.141084410135\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00240084688994 0.14270889718\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00240175021248 0.138639604415\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00240099791613 0.145501084765\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00236058336366 0.14704271404\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00236531348052 0.142781635985\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00236571689209 0.142294353565\n",
      "most recent validation loss: 0.00238187994539 0.141205496115\n",
      "training loss: 0.00234039268542 0.150593868822\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.0023347416149 0.13851312871\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.00232845469835 0.147055945701\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.00233131070502 0.143405880558\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.00230428106391 0.141031923622\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.00229564702165 0.140757231575\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.00229637263778 0.137610324293\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.00227384580692 0.14576338494\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.00225032627641 0.147126663311\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.00225247018379 0.143224111115\n",
      "most recent validation loss: 0.0024006848279 0.146367212137\n",
      "training loss: 0.00224359921149 0.144005231456\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00223580736219 0.144526920825\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00222413483548 0.141228056882\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00221743179637 0.140649980775\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00221804373551 0.145560047215\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00219602265642 0.147436742789\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00217234136245 0.146661957986\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00218132272831 0.14387862711\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00218610082754 0.137046747522\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00215818560721 0.14711319358\n",
      "most recent validation loss: 0.00228003840052 0.146367212137\n",
      "training loss: 0.00215877109892 0.141768570922\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.0021296894048 0.144715241965\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.00212824491244 0.139690122144\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.00212659609154 0.140923647438\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.00212627461517 0.138414175091\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.00213049582828 0.13904205084\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.00209090944112 0.140016592031\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.00208892469628 0.141802440822\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.00208661444942 0.145660001869\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.00207295944391 0.143615586936\n",
      "most recent validation loss: 0.00240651052932 0.145987853124\n",
      "training loss: 0.00205345038144 0.144428387756\n",
      "most recent validation loss: 0.00212899153173 0.147304998242\n",
      "training loss: 0.0020432723178 0.146781569405\n",
      "most recent validation loss: 0.00212899153173 0.147304998242\n",
      "training loss: 0.00203651930737 0.147202119376\n",
      "most recent validation loss: 0.00212899153173 0.147304998242\n",
      "training loss: 0.00203129281858 0.144935931559\n",
      "most recent validation loss: 0.00212899153173 0.147304998242\n",
      "training loss: 0.00202671551936 0.146302831252\n",
      "most recent validation loss: 0.00212899153173 0.147304998242\n",
      "training loss: 0.00202083374229 0.145796960004\n",
      "most recent validation loss: 0.00212899153173 0.147304998242\n",
      "training loss: 0.00199365768687 0.141752963353\n",
      "most recent validation loss: 0.00212899153173 0.147304998242\n",
      "training loss: 0.0019879185487 0.146795016966\n",
      "most recent validation loss: 0.00212899153173 0.147304998242\n",
      "training loss: 0.00198406433119 0.142806608889\n",
      "most recent validation loss: 0.00212899153173 0.147304998242\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       958\n",
      "          1       0.00      0.00      0.00       111\n",
      "          2       0.14      1.00      0.25      1024\n",
      "          3       0.00      0.00      0.00      1774\n",
      "          4       0.00      0.00      0.00      1247\n",
      "          5       0.00      0.00      0.00       831\n",
      "          6       0.00      0.00      0.00      1233\n",
      "\n",
      "avg / total       0.02      0.14      0.04      7178\n",
      "\n",
      "accuracy:  0.14265812204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natalia/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 200, 100, 50, (7, 'softmax')], weighted=True, loss ='hinge')\n",
    "for tr, val in net.itertrain((train[0], train[1], weights), (test[0], test[1], weights_t),  algo='nag', \n",
    "                             learning_rate=1e-3, momentum=0.9, weight_l1=10):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['loss'], val['acc'])\n",
    "    \n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poor performance, was stoped mannualy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.24542855599 0.192568679464\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 2.13224968444 0.235271359859\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 2.0583945648 0.262342149062\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 2.0144158836 0.282306388691\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 1.98435215374 0.296821163968\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 1.96409392997 0.302242174937\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 1.94709381296 0.308274286104\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 1.93555087392 0.314353638615\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 1.92093482197 0.320156651683\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 1.90844473221 0.330131221401\n",
      "most recent validation loss: 3.29142383713 0.141205496115\n",
      "training loss: 1.89998937773 0.32961007681\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.88691325392 0.335203148153\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.87718257429 0.341849177129\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.86539647927 0.344048823462\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.85912982767 0.347972264187\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.84106720878 0.354610985723\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.83518893507 0.358190836809\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.82079051488 0.363135560137\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.8122887128 0.36578477614\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.79954488548 0.374725023782\n",
      "most recent validation loss: 1.89596366507 0.321475115442\n",
      "training loss: 1.7861851859 0.378570085733\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.77457880972 0.386760834287\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.76407693478 0.388218164998\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.75053735368 0.394390499595\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.74217146039 0.395084245177\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.72516897155 0.400653669942\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.71729716845 0.402961249394\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.69788539067 0.4153371117\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.68587738154 0.420248285798\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.68411549772 0.418074547619\n",
      "most recent validation loss: 1.82695306067 0.35463067071\n",
      "training loss: 1.67264226672 0.423879450876\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.65636521059 0.429758800617\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.6410737016 0.438966353066\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.62960227204 0.443242866304\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.61974154888 0.447441671565\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.60219591244 0.450491323417\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.59119434832 0.452746305762\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.57846975313 0.460705656027\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.56860008212 0.466999020527\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.55483899808 0.470244079367\n",
      "most recent validation loss: 1.78689568948 0.379827767489\n",
      "training loss: 1.54738146827 0.473503530858\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.53647251461 0.476758293795\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.5112046672 0.486474670241\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.50631335106 0.486348233091\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.49182009381 0.494586112838\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.49010786239 0.494704994434\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.47170932076 0.501759633696\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.46091592911 0.503936800719\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.43091798375 0.514503442198\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.43784370472 0.514989363168\n",
      "most recent validation loss: 1.89353764063 0.369627509586\n",
      "training loss: 1.41139725538 0.523122241526\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.41673450466 0.520580393302\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.39481483046 0.531214829348\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.38645900146 0.534618318043\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.37282930566 0.536318576366\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.35485862776 0.54902544979\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.3472932527 0.549326756437\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.34112844648 0.55362399962\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.31719986377 0.56247704183\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.29102889805 0.570682875554\n",
      "most recent validation loss: 1.79523331491 0.405533204091\n",
      "training loss: 1.30145434719 0.567959252941\n",
      "most recent validation loss: 2.05702617842 0.4085246254\n",
      "training loss: 1.2949771758 0.572710260039\n",
      "most recent validation loss: 2.05702617842 0.4085246254\n",
      "training loss: 1.25882741609 0.584839752651\n",
      "most recent validation loss: 2.05702617842 0.4085246254\n",
      "training loss: 1.2429451885 0.59065462172\n",
      "most recent validation loss: 2.05702617842 0.4085246254\n",
      "training loss: 1.23003411047 0.595463545\n",
      "most recent validation loss: 2.05702617842 0.4085246254\n",
      "training loss: 1.24574092818 0.591687118774\n",
      "most recent validation loss: 2.05702617842 0.4085246254\n",
      "training loss: 1.24817765896 0.591149195921\n",
      "most recent validation loss: 2.05702617842 0.4085246254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-426b0fc9cc3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'most recent validation loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'classification_report:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/natalia/anaconda3/lib/python3.5/site-packages/theanets/feedforward.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mof\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone\u001b[0m \u001b[0mper\u001b[0m \u001b[0mrow\u001b[0m \u001b[0mof\u001b[0m \u001b[0minput\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         '''\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/natalia/anaconda3/lib/python3.5/site-packages/theanets/graph.py\u001b[0m in \u001b[0;36mfeed_forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m                 self.inputs, exprs, updates=updates))\n\u001b[0;32m    517\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/natalia/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = tn.Classifier(layers=[train[0].shape[1], 1000, 500, 100, (7, 'softmax')], weighted=True)\n",
    "for tr, val in net.itertrain((train[0], train[1], weights), (test[0], test[1], weights_t),  algo='nag', \n",
    "                             learning_rate = 1e-3, momentum = 0.9, weight_l1 = 10):\n",
    "    print('training loss:', tr['loss'], tr['acc'])\n",
    "    print('most recent validation loss:', val['loss'], val['acc'])\n",
    "    \n",
    "predictions = net.predict(test[0])\n",
    "print('classification_report:\\n', classification_report(test[1], predictions))\n",
    "print('accuracy: ', accuracy_score(test[1], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too long, therefore it  was interrupted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final remarks\n",
    "\n",
    "* Obtained results correspond to place 40-45 in the competition leaderbord.\n",
    "* Convolutional neural networks may improve the accuracy, but theanets has only 1D version and almost no documentation on them.\n",
    "* No sofisticated feature engineering was used in these experiments, but, may be, it also can affect performance.\n",
    "* Some classes are not enough recognisable even by human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
